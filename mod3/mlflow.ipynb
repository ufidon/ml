{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod3/mlflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod3/mlflow.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A complete ML procedure\n",
    "---\n",
    "_homl3 ch2_\n",
    "\n",
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore and visualize the data to gain insights.\n",
    "4. Prepare the data for machine learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Practice: Where to find datasets?\n",
    "---\n",
    "- [Explore the List of datasets for machine-learning research on Wikipedia]https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
    "- We will go through a complete ML procedure with the `California Housing Prices` dataset from the StatLib repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the big picture\n",
    "- Goal: use California census data to build a model of housing prices\n",
    "- The model should \n",
    "  - learn from this data \n",
    "  - be able to predict the median housing price in any district, \n",
    "    - given house conditions such as the population, median income, and median housing price for each block group in California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem\n",
    "- This is a typical `supervised learning task`\n",
    "  - since the model can be trained with labeled examples\n",
    "- This is a `multiple regression problem`\n",
    "  - since the system will use multiple features to make a prediction\n",
    "- It is also a `univariate regression problem`\n",
    "  - since we are only trying to predict a single value for each district\n",
    "- the data is small enough to fit in memory\n",
    "  - so plain batch learning should do just fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Performance Measure\n",
    "Two typical performance measures for regression problems are\n",
    "1. the root mean square error (RMSE) given $n$ samples\n",
    "\n",
    "$\\displaystyle \\operatorname{RMSE}(\\mathbf{X},h) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (h(\\mathbf{x}^{(i)})-y^{(i)})^2}$\n",
    "\n",
    "- $\\mathbf{x}^{(i)}$ is the $i^{th}$ instance in the dataset\n",
    " - it is a vector of all the feature values\n",
    "- $y^{(i)}$ is $\\mathbf{x^{(i)}}$'s label, the desired output value for this instance\n",
    "- $\\mathbf{X}$ is the instance matrix, contains each instance as a row\n",
    "- $h$ is the ML model's prediction function\n",
    "- $\\hat{y}^{(i)} = h(\\mathbf{x}^{(i)})$ is the predicted value of $\\mathbf{x}^{(i)}$\n",
    " - with prediction error of $\\hat{y}^{(i)}-y^{(i)}$\n",
    "\n",
    "2. mean absolute value (MAE), also called average absolute deviation\n",
    "\n",
    "$\\displaystyle \\operatorname{MAE}(\\mathbf{X},h) = \\frac{1}{n}\\sum_{i=1}^n \\left|(h(\\mathbf{x}^{(i)})-y^{(i)})^2\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distance between two vectors\n",
    "- RMSE is the Euclidean norm, the normal distance between two vectors\n",
    "  - also called $\\ell_2$ norm, noted $||\\cdot||_2$ or just $||\\cdot||$\n",
    "- MAE is the $\\ell_1$ norm, noted $||\\cdot||_1$ or just $|\\cdot|$\n",
    "- the general $\\ell_p$ norm is defined as $\\displaystyle ||\\mathbf{v}||_p=\\left(\\sum_{i=1}^m |v_i|^p\\right)^\\frac{1}{p}$\n",
    "  - $\\ell_0$ norm gives the number of $\\mathbf{v}$'s nonzero components\n",
    "  - $\\ell_\\infty$ norm gives $\\mathbf{v}$'s maximum absolute component\n",
    "- The higher the norm index $p$, the larger values are more significant\n",
    "  - so RMSE is more sensitive to outliers than MAE\n",
    "  - but when outliers are exponentially rare like in a bell-shaped distribution,\n",
    "    - the RMSE generally performs well and is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs regression\n",
    "- If the house prices are required to be partitioned into categories such as\n",
    "  - expensive, medium and cheap\n",
    "  - then this becomes a classification problem \n",
    "    - and predicting the price perfectly accurate is unimportant\n",
    "- In this ML flow, actual prices are needed so it is a regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the data\n",
    "housing = pd.read_csv(\"../datasets/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. peek first 5 rows in the dataset\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. get a quick description of the data\n",
    "# pay attention to the attributes\n",
    "\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pay attention to missing data such as \n",
    "  - total_bedrooms has 207=20640-20433 null values\n",
    "- pay attention to data types such as\n",
    "  - ocean_proximity has a data type object\n",
    "    - it is probably a categorical attribute based on the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. find all categories of a categorical attribute\n",
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. show a statistic summary of the numerical attributes\n",
    "# a percentile indicates the value below which \n",
    "# a given percentage of observations fall\n",
    "\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. find the value distribution of each numerical attribute\n",
    "#   with histogram\n",
    "#  A histogram shows the number of instances (on the vertical axis) \n",
    "#   that fall in a given value range (on the horizontal axis)\n",
    "fig1, axes1 = plt.subplots(3,3,figsize=(12,8))\n",
    "housing.hist(bins=50, ax= axes1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do the values above  make sense?\n",
    "  - median income attribute, only 0-15? what unit?\n",
    "    - scaled and capped to between [0.5, 15]\n",
    "    - unit: tens of thousands of dollars\n",
    "    - ‚à¥ the median income is between [$5000, $150,000]\n",
    "  - The housing median age and the median house value were also capped\n",
    "    - the median house value is our target attribute, or label\n",
    "  - These attributes have very different scales\n",
    "    - feature scaling is needed\n",
    "  - many histograms are skewed right\n",
    "    - need to transform these attributes to have more symmetrical and bell-shaped distributions\n",
    "- How do we know the hidden information?\n",
    "  - ask the dataset collectors and publishers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set\n",
    "- thumb rule for splitting the dataset: \n",
    "  - 80% for training and 20% for test\n",
    "- Test set generation is a critical part of a ML project\n",
    "  - but it is often neglected, which incurs bad even useless ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. shuffle then split data\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "  shuffled_indices = np.random.permutation(len(data))\n",
    "  test_set_size = int(len(data) * test_ratio)\n",
    "  test_indices = shuffled_indices[:test_set_size]\n",
    "  train_indices = shuffled_indices[test_set_size:]\n",
    "  return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issues: \n",
    "  - the random shuffling will generate different training set and test set every time\n",
    "  - overtime, the ML model will see the whole dataset\n",
    "- Tentative solutions:\n",
    "  1. save the test set on the first run and then load it in subsequent runs\n",
    "  2. fix the random number generator‚Äôs seed so that \n",
    "     - it always generates the same shuffled indices \n",
    "  - Problem: both these solutions will break the next time for an updated dataset\n",
    "- Common solution:\n",
    "  - use each instance‚Äôs identifier to decide whether or not it should go in the test set\n",
    "    - assuming instances have unique and immutable identifiers\n",
    "    - here, these identifiers are like container ids\n",
    "  - make these identifiers comparable such as hashing them\n",
    "    - then let the test set contains instances whose hashes no larger than 20% of the maximum hash value\n",
    "    - This ensures that the test set will remain consistent across multiple runs\n",
    "      - even the dataset is updated\n",
    "- These purely random sampling methods are generally fine for dataset large enough\n",
    "  - i.e. number of rows ‚â´ number of features\n",
    "  - otherwise, a significant sampling bias could be introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. use hash to ensures that \n",
    "# the test set will remain consistent across multiple runs\n",
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    " \n",
    "# The housing dataset does not have an identifier column. \n",
    "#  2.1 A simplest solution is to use the row index as the ID  \n",
    "# But this requires that \n",
    "#   new data gets appended to the end of the dataset \n",
    "#   and that no row ever gets deleted\n",
    "housing_with_id = housing.reset_index()  # adds an `index` column\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")  \n",
    "\n",
    "#  2.2 Another solution is \n",
    "#   combining district‚Äôs latitude and longitude into an ID like\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways\n",
    "- `train_test_split()` is the simplest function among them\n",
    "  - similar to the house-made `shuffle_and_split_data()` above\n",
    "  - but with a couple of additional features\n",
    "    - it has  a `random_state` parameter for setting the random generator seed\n",
    "    - it can split multiple datasets of the same size on the same indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. split dataset with train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the split randomness looks uniform and acceptable\n",
    "test_set[\"total_bedrooms\"].isnull().sum()/housing[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling\n",
    "- divides the population into homogeneous subgroups called strata\n",
    "- then samples the right number of instances from each stratum\n",
    "- guarantees that the test set is representative of the overall population\n",
    "- In a dataset, \n",
    "  - it is important to have a sufficient number of instances for each stratum\n",
    "  - otherwise, the estimate of a stratum‚Äôs importance may be biased\n",
    "- `sklearn.model_selection` package provides a number of splitter classes\n",
    "- Each splitter has a `split()` method that returns an iterator \n",
    "  - over different training/test splits of the same data\n",
    "  - it yields the training and test indices, not the data itself\n",
    "  - this is very useful for cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling\n",
    "# 1. the US population is 51.1% females and 48.9% males, \n",
    "#   so a well-conducted survey in the US would try to maintain \n",
    "#   this ratio in the sample: 511 females and 489 males\n",
    "\n",
    "# 1.1 the probability of getting a biased sample with \n",
    "#   <48.5% female or\n",
    "#   >53.5% female\n",
    "# The `cdf()` method of the binomial distribution gives us \n",
    "# the probability of `the number of females ‚â§ the given value`\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)\n",
    "proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)\n",
    "print(proba_too_small + proba_too_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Obtain the same result by simulation\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Stratify the median incomes in the California house dataset\n",
    "#   create an income category attribute with five categories \n",
    "#   labeled from 1 to 5\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 visualize the income categories\n",
    "\n",
    "axes2 = housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "axes2.set_xlabel(\"Income category\")\n",
    "axes2.set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generates 10 different splits of the California house dataset\n",
    "#    stratified on the income categories\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "strat_splits = []\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set_n = housing.iloc[train_index]\n",
    "    strat_test_set_n = housing.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 use the last split\n",
    "strat_train_set, strat_test_set = strat_splits[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 a short way to get a single stratified split\n",
    "#   with `train_test_split`\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 check if the stratified split worked as expected\n",
    "#   check the similarity of this figure with the previous one\n",
    "\n",
    "axes3 = (strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)).sort_index().plot.bar(rot=0, grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 compares the income category proportions in  \n",
    "#   the overall dataset, \n",
    "#   the test set generated with stratified sampling, \n",
    "#   the test set generated using purely random sampling\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall %\": income_cat_proportions(housing),\n",
    "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
    "    \"Random %\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props.index.name = \"Income Category\"\n",
    "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
    "                                   compare_props[\"Overall %\"] - 1)\n",
    "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
    "                                  compare_props[\"Overall %\"] - 1)\n",
    "(compare_props * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows\n",
    "- the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, \n",
    "- whereas the test set generated using purely random sampling is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 the income_cat column can be dropped if it is not used again\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "  set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Visualize the Data to Gain Insights\n",
    "- put the test set aside and explore the training set only\n",
    "- better work on a copy of the training set \n",
    "  - since various transformations will be experimented\n",
    "- sample an exploration set if the training set is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make a copy of the original training set since it is small\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Geographical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 the data point distribution looks like California\n",
    "\n",
    "fig4, axes4 = plt.subplots(figsize=(6,4))\n",
    "housing.plot(ax=axes4, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 visualize data point density with partially transparency\n",
    "# What are those areas of high density?\n",
    "\n",
    "fig5, axes5 = plt.subplots(figsize=(6,4))\n",
    "housing.plot(ax=axes5, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. visualize \n",
    "#   housing prices with pseudo-colors\n",
    "#   district's population with circle sizes\n",
    "# It shows the housing prices are very much related to \n",
    "#   the location and \n",
    "#   the population density\n",
    "\n",
    "fig6, axes6 = plt.subplots(figsize=(10,7))\n",
    "housing.plot(ax=axes6, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. visualize house price and district's population on the map of California\n",
    "filename = \"../datasets/california.png\"\n",
    "\n",
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (·¥ús·¥Ö)\"})\n",
    "\n",
    "fig7, axes7 = plt.subplots(figsize=(10,7))\n",
    "housing_renamed.plot(ax=axes7,\n",
    "             kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n",
    "             s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "             c=\"Median house value (·¥ús·¥Ö)\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False)\n",
    "\n",
    "california_img = plt.imread( filename)\n",
    "axis = (-124.55, -113.95, 32.45, 42.05)\n",
    "axes7.axis(axis)\n",
    "axes7.imshow(california_img, extent=axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for [Correlations](https://en.wikipedia.org/wiki/Correlation) between features\n",
    "- The correlation coefficient only measures linear correlations, such as\n",
    "  - x goes up, y generally goes up/down\n",
    "  - this has nothing to do with the slope\n",
    "- It may completely miss out on nonlinear relationships\n",
    "- Purposes\n",
    "  - identify and clean outliers\n",
    "  - identify and transform skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute the standard correlation coefficient between every pair of attributes\n",
    "#     also called Pearson‚Äôs r\n",
    "corr_matrix = housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 find out how much each attribute correlates with the median house value\n",
    "# Explain\n",
    "#   - positive correlation, negative correlation and no correlation\n",
    "\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. check for correlation between attributes \n",
    "#     using the Pandas scatter_matrix() function\n",
    "# It plots every numerical attribute against every other numerical attribute. \n",
    "#    - Since there are now 11 numerical attributes, \n",
    "#       - you would get 11¬≤ = 121 plots\n",
    "#    - we may choose a few attributes that seem \n",
    "#       - most correlated with the median housing value\n",
    "# Note: Pandas displays a histogram of each attribute on the main diagonal\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "\n",
    "fig8, axes8 = plt.subplots(4,4, figsize=(12, 8))\n",
    "scatter_matrix(housing[attributes], ax=axes8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there is a strong positive correlation between `median_income` and `median_house_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. zoom in the correlation between `median_income` and `median_house_value`\n",
    "# there are several horizontal lines in the figure\n",
    "# the most obvious horizontal line is at $500,000,\n",
    "#   which is the price cap\n",
    "# It is better remove the corresponding districts from the data set\n",
    "\n",
    "fig9, axes9 = plt.subplots(figsize=(6, 4))\n",
    "housing.plot(ax=axes9, kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1, grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Attribute Combinations\n",
    "- attribute combinations could be more meaningful than their attributes alone\n",
    "- such as,\n",
    "  - the number of rooms per household\n",
    "  - the population per household\n",
    "  - the ratio of the number of bedrooms to the number of rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Attribute combination\n",
    "\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. verify the correlations between the attribute combinations and the target\n",
    "# Are they more correlated to the target than their component attributes?\n",
    "#  - bedrooms_ratio vs. total_rooms or total_bedrooms\n",
    "#  - rooms_per_house vs. total_rooms or households\n",
    "#  - people_per_house vs. population or households\n",
    "\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. revert to a clean training set\n",
    "#   - separate the predictors and the labels\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # no change in place\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "- Three options to an attributes with some missing values\n",
    "  1. drop the missing values with `dropna()`\n",
    "  2. remove the whole attribute with `drop()`\n",
    "  3. fill the missing values with some values such as zero, the mean, the median, etc.\n",
    "     - with `fillna()`\n",
    "     - this is called `imputation`\n",
    "- use `SimpleImputer` for option 3 instead of `fillna()`, because `SimpleImputer` has more features\n",
    "  - it will store the median value of each feature\n",
    "  - it is possible to impute missing values on \n",
    "    - the training set, the validation set, \n",
    "    - the test set, and any new data fed to the model\n",
    "- other more powerful imputers in sklearn.impute package, to replace missing values\n",
    "  - `KNNImputer` uses the mean of the k-nearest neighbors‚Äô values of that feature\n",
    "  - `IterativeImputer` uses predicted missing values with a regression model per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rows that originally contained a NaN value\n",
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "housing.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: drop the missing values\n",
    "\n",
    "housing_option1 = housing.copy()\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  \n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: remove the whole attribute, i.e. column\n",
    "housing_option2 = housing.copy()\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True) \n",
    "housing_option2.loc[null_rows_idx].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: fill the missing values with some values such as zero, the mean, the median, etc.\n",
    "housing_option3 = housing.copy()\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  \n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create an imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\") # replace NaNs with the median of the attribute\n",
    "# other strategies: mean, most_frequent, constant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 strategy=\"median\" only works ono numerical attributes\n",
    "# create a copy of the data with only the numerical attributes\n",
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 fit the imputer instance to the training data\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 the imputer simply computed the median of each attribute and\n",
    "#  stored the result in its statistics_ instance variable.\n",
    "#  It is safer to apply the imputer to all the numerical attributes\n",
    "#  preparing for any missing values in new data.\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the above results with\n",
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 use this ‚Äútrained‚Äù imputer to transform the training set\n",
    "#   by replacing missing values with the learned medians\n",
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 X is a NumPy array, \n",
    "# it can be wrapped in a DataFrame and \n",
    "#   recover the column names and index from housing_num.\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Predict outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 Drop outliers\n",
    "# uncomment and run these codes if you want to drop outliers\n",
    "#housing = housing.iloc[outlier_pred == 1]\n",
    "#housing_labels = housing_labels.iloc[outlier_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "- The values of categorical attributes are limited\n",
    "- They can be encoded in integers with `OrdinalEncoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. There is only one text attribute `ocean_proximity` in housing\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. `ocean_proximity` has just 5 different values\n",
    "#  each of which represents a category, so it is a categorical attribute.\n",
    "# Convert these categories from text to numbers \n",
    "#   since most machine learning algorithms prefer to work with numbers.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded[:5], housing_cat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issue of `OrdinalEncoder`: ML algorithms will assume that two nearby values are more similar than two distant values\n",
    "  - fine with ordered categories such as ‚Äúbad‚Äù, ‚Äúaverage‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù\n",
    "  - but bad with unordered categories such USA state names\n",
    "- `one-hot encoding` can fix this issue\n",
    "  - implemented in sklearn class `OneHotEncoder`\n",
    "  - it encodes the categories with a binary string\n",
    "    - the length of the binary string equals the number of categories, \n",
    "    - each code has only one bit that is set to 1 for the coded category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. encode `ocean_proximity` categories with `OneHotEncoder`\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the `OneHotEncoder` class returns a sparse array,\n",
    "# because it is  a very efficient representation for matrices that contain mostly zeros,\n",
    "#  it only stores the nonzero values and their positions.\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert it to a dense array if needed by calling the `toarray()` method:\n",
    "housing_cat_1hot.toarray()[:5] # compare it with housing_cat_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or specify that `OneHotEncoder` class returns a dense array\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issue of `OneHotEncoder`\n",
    "  - results in a large number of input features if the categorical attribute has a large number of possible categories\n",
    "  - i.e. the binary code is very long\n",
    "  - This may slow down training and degrade performance\n",
    "- Possible solutions:\n",
    "  - replace the categorical input with useful numerical features related to the categories\n",
    "    - e.x.  replace the `ocean_proximity` feature with the distance to the ocean\n",
    "  - replace each category with a learnable, low-dimensional vector called an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Transformation\n",
    "- Feature scaling is one of the most important transformations on data\n",
    "  - Most ML algorithms don't perform well on numerical attributes with very different scales\n",
    "  - they bias toward the features with large scales\n",
    "- Two common feature scaling techniques:\n",
    "  -  `min-max scaling` implemented in sklearn class `MinMaxScaler`\n",
    "     - $\\displaystyle v'=\\frac{v-v_{min}}{v_{max}-v_{min}}$\n",
    "  -  `standardization` in `StandardScaler`\n",
    "     - $\\displaystyle v'=\\frac{v-v_{mean}}{œÉ_v}$\n",
    "     - $œÉ_v$ is the standard deviation\n",
    "- standardization \n",
    "  - does not restrict values to a specific range\n",
    "  - is much less affected by outliers\n",
    "  - will break a sparse matrix unless set its `with_mean` hyperparameter to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. `min-max` scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. `standardization` scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Long-tail distribution is generally transformed to be roughly symmetrical for better ML training\n",
    "  - but both min-max scaling and standardization will squash most values into a small range\n",
    "- Ways to shrink the heavy tail before scaling\n",
    "  - replace the feature with its square root\n",
    "    - or raise the feature to a power between 0 and 1\n",
    "  - replace the feature under a [power law distribution](https://en.wikipedia.org/wiki/Power_law) by its logarithm\n",
    "  - bucket the feature\n",
    "    - chop its distribution into roughly equal-sized buckets, \n",
    "    - and replace each feature value with the index of the bucket it belongs to\n",
    "    - divide by the number of buckets to force the values to the 0‚Äì1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. replace the feature under a power law distribution\n",
    "# by its logarithm\n",
    "\n",
    "figa, axesa = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "housing[\"population\"].hist(ax=axesa[0], bins=50)\n",
    "housing[\"population\"].apply(np.log).hist(ax=axesa[1], bins=50)\n",
    "axesa[0].set(xlabel=\"Population\", title='A long tail')\n",
    "axesa[1].set(xlabel=\"Log of population\", title='Close to normal distribution')\n",
    "axesa[0].set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. replace each value with its percentile\n",
    "# Bucketizing with equal-sized buckets results in \n",
    "# a feature with an almost uniform distribution.\n",
    "\n",
    "percentiles = [np.percentile(housing[\"median_income\"], p)\n",
    "               for p in range(1, 100)]\n",
    "flattened_median_income = pd.cut(housing[\"median_income\"],\n",
    "                                 bins=[-np.inf] + percentiles + [np.inf],\n",
    "                                 labels=range(1, 100 + 1))\n",
    "figb, axesb = plt.subplots(1, 1, figsize=(5, 3))\n",
    "flattened_median_income.hist(bins=50, ax=axesb)\n",
    "axesb.set_xlabel(\"Median income percentile\")\n",
    "axesb.set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A multimodal distribution has two or more clear peaks, called modes\n",
    "- Bucket can also be used on a feature with a multimodal distribution\n",
    "  - treat the bucket IDs as categories, rather than as numerical values\n",
    "    - the bucket indices must be encoded such as using `OneHotEncoder`\n",
    "  - this allows the regression model to learn different rules for different ranges of this feature value\n",
    "- Another approach to transforming multimodal distributions is to\n",
    "  - add a feature for each of the modes, at least the main ones\n",
    "    - representing the similarity between the original feature median and that particular mode\n",
    "  - Gaussian radial basis function (RBF) is typically used to measure the distance between the input value and a fixed point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a new Gaussian RBF feature measuring \n",
    "# the similarity between the housing median age and 35\n",
    "\n",
    "# The hyperparameter Œ≥ (gamma) determines how quickly \n",
    "# the similarity measure decays as x moves away from the reference 35\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. visualize the new similarity feature \n",
    "#     created by Gaussian RBF with two different Œ≥'s\n",
    "# if this particular age group around 35 is well correlated with lower prices, \n",
    "# there‚Äôs a good chance that this new feature will help.\n",
    "\n",
    "ages = np.linspace(housing[\"housing_median_age\"].min(),\n",
    "                   housing[\"housing_median_age\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\n",
    "rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n",
    "\n",
    "figc, axesc = plt.subplots()\n",
    "\n",
    "axesc.set_xlabel(\"Housing median age\")\n",
    "axesc.set_ylabel(\"Number of districts\")\n",
    "axesc.hist(housing[\"housing_median_age\"], bins=50)\n",
    "axesc.vlines([35], [0], [1000], ['red'], ['dotted'])\n",
    "\n",
    "ax2 = axesc.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(ages, rbf1, color=color, label=f\"gamma = {gamma1}\")\n",
    "ax2.plot(ages, rbf2, color=color, label=f\"gamma = {gamma2}\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Age similarity\", color=color)\n",
    "\n",
    "ax2.legend(loc=\"upper left\");\n",
    "ax2.set_title('The Similarity Decays Slower With Smaller Œ≥');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Scaling And Transformation\n",
    "- if the target distribution has a heavy tail,\n",
    "  - it can be replaced with its logarithm as well\n",
    "  - but the regression model will now predict the log of the target\n",
    "    - so we need to compute the exponential of the model‚Äôs prediction to get the predicted target\n",
    "      - this can be done with transformer method `inverse_transform()`\n",
    "- A shortcut for the same purpose\n",
    "  - `TransformedTargetRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. scale and transform the target - house price\n",
    "# for a final prediction, `inverse_transform` is called\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
    "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_predictions.round(2), predictions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Exploit the shortcut `TransformedTargetRegressor`\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model = TransformedTargetRegressor(LinearRegression(),\n",
    "                                   transformer=StandardScaler())\n",
    "model.fit(housing[[\"median_income\"]], housing_labels)\n",
    "predictions = model.predict(some_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "- are used for custom transformations, cleanup operations, or combining specific attributes\n",
    "- A custom class is needed for \n",
    "  - learning some parameters in the `fit()` method, and\n",
    "  - using them later in the `transform()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. A simple log transformers\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# the `inverse_func` is optional unless \n",
    "# you use this transformer in a TransformedTargetRegressor\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[[\"population\"]][:2], log_pop[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. An RBF transformer used to create \n",
    "# a new feature from one old feature, \n",
    "# the distance between an old value and the landmark\n",
    "landmark = [35.]\n",
    "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                      kw_args=dict(Y=[landmark], gamma=0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[[\"housing_median_age\"]][:2], age_simil_35[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. An RBF transformer used to create \n",
    "# a new feature from two old features with 2D distance\n",
    "\n",
    "landmark = (37.7749, -122.41)\n",
    "sf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                     kw_args=dict(Y=[landmark], gamma=0.1))\n",
    "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[[\"latitude\", \"longitude\"]][:2], sf_simil[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Custom transformers are also useful to combine features\n",
    "# such as computing the ratio between the input features 0 and 1\n",
    "\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.  a custom transformer that acts much like the `StandardScaler`\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "# TransformerMixin as a base class: \n",
    "#   the default implementation will just call fit() and then transform()\n",
    "# BaseEstimator as a base class:\n",
    "#   get two extra methods: get_params() and set_params()\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. A custom transformer can use other estimators in its implementation\n",
    "# such as using a KMeans clusterer \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 use the custom transformer\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
    "                                           sample_weight=housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result: one row per district, \n",
    "# and one column per cluster, 10 columns for 10 clusters\n",
    "similarities[:3].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 visualize the 10 cluster centers found by k-means\n",
    "#   - most clusters are located in highly populated and expensive areas\n",
    "#   - the districts are colored according to their geographic \n",
    "#       similarity to their closest cluster center\n",
    "\n",
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (·¥ús·¥Ö)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
    "\n",
    "figd, axesd = plt.subplots()\n",
    "housing_renamed.plot(ax=axesd, kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
    "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(10, 7))\n",
    "axesd.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
    "         label=\"Cluster centers\")\n",
    "axesd.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Pipelines\n",
    "- organize sequences of transformations\n",
    "- can be created with \n",
    "  - the `Pipeline` class\n",
    "  - the `make_pipeline()` function\n",
    "- The Pipeline constructor takes a list of name/estimator pairs\n",
    "  - The estimators must all be transformers\n",
    "    - i.e., they must have a fit_transform() method\n",
    "  - except for the last one, which can be anything: \n",
    "    - a transformer, a predictor, or any other type of estimator\n",
    "- `make_pipeline()` takes transformers as positional arguments\n",
    "  - creates a Pipeline using the names of the transformers‚Äô classes\n",
    "  - If multiple transformers have the same name, an index is appended\n",
    "- The pipeline‚Äôs fit() method\n",
    "  - calls `fit_transform()` sequentially on all the transformers\n",
    "  - calls `fit()` on the final estimator\n",
    "- The pipeline exposes the same methods as the final estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  a small pipeline for numerical attributes by `Pipeline` class, \n",
    "#   which will first impute then scale the input features\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"standardize\", StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 by `make_pipeline` without naming the transformers explicitly\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. call num_pipeline.fit_transform\n",
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_prepared[:2].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 recover a DataFrame using the pipeline‚Äôs get_feature_names_out()\n",
    "df_housing_num_prepared = pd.DataFrame(\n",
    "    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n",
    "    index=housing_num.index)\n",
    "\n",
    "df_housing_num_prepared.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. access estimators in a pipeline with \n",
    "#   - index, or\n",
    "#   - the steps attribute, or\n",
    "#   - the named_steps dictionary attribute\n",
    "\n",
    "num_pipeline[1], num_pipeline.steps[1], num_pipeline.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ColumnTransformer` applies the appropriate transformations to each column adaptively\n",
    "  - `num_pipeline` on the numerical columns\n",
    "  - `cat_pipeline` on the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Exploit `ColumnTransformer`\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Exploit `make_column_transformer()` to \n",
    "#   create a ColumnTransformer without naming each transformer explicitly\n",
    "\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Transform all housing columns\n",
    "housing_prepared = preprocessing.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Build a DataFrame for the transformed housing\n",
    "housing_prepared_fr = pd.DataFrame(\n",
    "    housing_prepared,\n",
    "    columns=preprocessing.get_feature_names_out(),\n",
    "    index=housing.index)\n",
    "housing_prepared_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. put all the previous transforms together\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler())\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler())\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "                               \"households\", \"median_income\"]),\n",
    "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 apply the synthesized ColumnTransformer on housing\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 show all final features\n",
    "preprocessing.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a model and train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start from a basic linear regression model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 try the trained model on the first five instances\n",
    "housing_predictions = lin_reg.predict(housing)\n",
    "housing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Compare against the actual values:\n",
    "housing_labels.iloc[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 What are the relative prediction errors?\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 measure this regression model‚Äôs RMSE on the whole training set\n",
    "#   using Scikit-Learn‚Äôs mean_squared_error() function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "lin_rmse, lin_rmse / housing_labels.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The relative prediction error is about 33.27%\n",
    "  - this signs the model underfitting the training data\n",
    "  - Ways to fix underfitting\n",
    "    - select a more powerful model\n",
    "    - feed the training algorithm with better features\n",
    "    - reduce the constraints on the model if it has\n",
    "      - not the case for the basic linear regression model used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Try a fairly powerful model - `DecisionTreeRegressor`\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Evaluate the tree on the training set\n",
    "housing_predictions = tree_reg.predict(housing)\n",
    "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                              squared=False)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No error!\n",
    "  - A sign of overfitting or inappropriate evaluation\n",
    "- Better evaluate it again with `Cross-Validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Evaluate the tree again with `Cross-Validation`\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the scores\n",
    "pd.Series(tree_rmses).describe()\n",
    "# The result is  as poor as the linear regression model\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Try a more powerful model `RandomForestRegressor`\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = make_pipeline(preprocessing,\n",
    "                           RandomForestRegressor(random_state=42))\n",
    "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
    "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Check the validation scores again\n",
    "pd.Series(forest_rmses).describe()\n",
    "# much better now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Let's compare this \n",
    "#   RMSE measured using cross-validation (the \"validation error\") with \n",
    "#   the RMSE measured on the training set (the \"training error\"):\n",
    "\n",
    "forest_reg.fit(housing, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing)\n",
    "forest_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
    "                                 squared=False)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training error is much lower than the validation error\n",
    "  - A sign of overfitting\n",
    "  - or, there's a mismatch between the training data and the validation data\n",
    "    - this is NOT the case here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Present your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn design principles\n",
    "- `Consistency`: all objects share a consistent and simple interface\n",
    "  - `Estimators`, such as `SimpleImputer`, \n",
    "    - estimates some parameters based on a dataset by the `fit()` methods\n",
    "    - takes a dataset as a parameter, or dataset and labels for supervised learning algorithms\n",
    "    - guides the estimation process with hyperparameters\n",
    "      - such as a SimpleImputer‚Äôs `strategy`\n",
    "  - `Transformers`, a special type of estimator, also called transformers\n",
    "    - transforms a dataset by the `transform()` method\n",
    "    - relies on the learned parameters\n",
    "    - has a optimized `fit_transform()` method that calls `fit()` then `transform()`\n",
    "  - `Predictors`, a special type of estimator, such as `LinearRegression`\n",
    "    - also called `predictors`\n",
    "    - makes predictions on new instances by the `predict()` method\n",
    "    - returns corresponding predictions of given new instances\n",
    "    - measures the quality of the predictions by the `score()` methods\n",
    "- `Inspection` exposes estimators' internal information via public instance variables, such as\n",
    "  - hyperparameters such as `imputer.strategy`\n",
    "  - learned parameters such as `imputer.statistics_`\n",
    "- `Nonproliferation of classes` reuses mature packages, such as\n",
    "  - Datasets are represented as NumPy arrays or SciPy sparse matrices\n",
    "  - Hyperparameters are just regular Python strings or numbers\n",
    "- `Composition` reuses existing building blocks as much as possible, such as \n",
    "  - a Pipeline estimator is composed of an arbitrary sequence of transformers followed by a final estimator\n",
    "- `Sensible defaults` provides reasonable default values for most parameters\n",
    "  - quick for prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [sklearn user guide](https://scikit-learn.org/stable/user_guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
