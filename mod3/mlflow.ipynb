{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod3/mlflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod3/mlflow.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A complete ML procedure\n",
    "---\n",
    "_homl3 ch2_\n",
    "\n",
    "1. Look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore and visualize the data to gain insights.\n",
    "4. Prepare the data for machine learning algorithms.\n",
    "5. Select a model and train it.\n",
    "6. Fine-tune your model.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù Practice: Where to find datasets?\n",
    "---\n",
    "- [Explore the List of datasets for machine-learning research on Wikipedia]https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
    "- We will go through a complete ML procedure with the `California Housing Prices` dataset from the StatLib repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the big picture\n",
    "- Goal: use California census data to build a model of housing prices\n",
    "- The model should \n",
    "  - learn from this data \n",
    "  - be able to predict the median housing price in any district, \n",
    "    - given house conditions such as the population, median income, and median housing price for each block group in California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame the Problem\n",
    "- This is a typical `supervised learning task`\n",
    "  - since the model can be trained with labeled examples\n",
    "- This is a `multiple regression problem`\n",
    "  - since the system will use multiple features to make a prediction\n",
    "- It is also a `univariate regression problem`\n",
    "  - since we are only trying to predict a single value for each district\n",
    "- the data is small enough to fit in memory\n",
    "  - so plain batch learning should do just fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Performance Measure\n",
    "Two typical performance measures for regression problems are\n",
    "1. the root mean square error (RMSE) given $n$ samples\n",
    "\n",
    "$\\displaystyle \\operatorname{RMSE}(\\mathbf{X},h) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (h(\\mathbf{x}^{(i)})-y^{(i)})^2}$\n",
    "\n",
    "- $\\mathbf{x}^{(i)}$ is the $i^{th}$ instance in the dataset\n",
    " - it is a vector of all the feature values\n",
    "- $y^{(i)}$ is $\\mathbf{x^{(i)}}$'s label, the desired output value for this instance\n",
    "- $\\mathbf{X}$ is the instance matrix, contains each instance as a row\n",
    "- $h$ is the ML model's prediction function\n",
    "- $\\hat{y}^{(i)} = h(\\mathbf{x}^{(i)})$ is the predicted value of $\\mathbf{x}^{(i)}$\n",
    " - with prediction error of $\\hat{y}^{(i)}-y^{(i)}$\n",
    "\n",
    "2. mean absolute value (MAE), also called average absolute deviation\n",
    "\n",
    "$\\displaystyle \\operatorname{MAE}(\\mathbf{X},h) = \\frac{1}{n}\\sum_{i=1}^n \\left|(h(\\mathbf{x}^{(i)})-y^{(i)})^2\\right|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distance between two vectors\n",
    "- RMSE is the Euclidean norm, the normal distance between two vectors\n",
    "  - also called $\\ell_2$ norm, noted $||\\cdot||_2$ or just $||\\cdot||$\n",
    "- MAE is the $\\ell_1$ norm, noted $||\\cdot||_1$ or just $|\\cdot|$\n",
    "- the general $\\ell_p$ norm is defined as $\\displaystyle ||\\mathbf{v}||_p=\\left(\\sum_{i=1}^m |v_i|^p\\right)^\\frac{1}{p}$\n",
    "  - $\\ell_0$ norm gives the number of $\\mathbf{v}$'s nonzero components\n",
    "  - $\\ell_\\infty$ norm gives $\\mathbf{v}$'s maximum absolute component\n",
    "- The higher the norm index $p$, the larger values are more significant\n",
    "  - so RMSE is more sensitive to outliers than MAE\n",
    "  - but when outliers are exponentially rare like in a bell-shaped distribution,\n",
    "    - the RMSE generally performs well and is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs regression\n",
    "- If the house prices are required to be partitioned into categories such as\n",
    "  - expensive, medium and cheap\n",
    "  - then this becomes a classification problem \n",
    "    - and predicting the price perfectly accurate is unimportant\n",
    "- In this ML flow, actual prices are needed so it is a regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the data\n",
    "housing = pd.read_csv(\"../datasets/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. peek first 5 rows in the dataset\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. get a quick description of the data\n",
    "# pay attention to the attributes\n",
    "\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pay attention to missing data such as \n",
    "  - total_bedrooms has 207=20640-20433 null values\n",
    "- pay attention to data types such as\n",
    "  - ocean_proximity has a data type object\n",
    "    - it is probably a categorical attribute based on the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. find all categories of a categorical attribute\n",
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. show a statistic summary of the numerical attributes\n",
    "# a percentile indicates the value below which \n",
    "# a given percentage of observations fall\n",
    "\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. find the value distribution of each numerical attribute\n",
    "#   with histogram\n",
    "#  A histogram shows the number of instances (on the vertical axis) \n",
    "#   that fall in a given value range (on the horizontal axis)\n",
    "fig1, axes1 = plt.subplots(3,3,figsize=(12,8))\n",
    "housing.hist(bins=50, ax= axes1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do the values above  make sense?\n",
    "  - median income attribute, only 0-15? what unit?\n",
    "    - scaled and capped to between [0.5, 15]\n",
    "    - unit: tens of thousands of dollars\n",
    "    - ‚à¥ the median income is between [$5000, $150,000]\n",
    "  - The housing median age and the median house value were also capped\n",
    "    - the median house value is our target attribute, or label\n",
    "  - These attributes have very different scales\n",
    "    - feature scaling is needed\n",
    "  - many histograms are skewed right\n",
    "    - need to transform these attributes to have more symmetrical and bell-shaped distributions\n",
    "- How do we know the hidden information?\n",
    "  - ask the dataset collectors and publishers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set\n",
    "- thumb rule for splitting the dataset: \n",
    "  - 80% for training and 20% for test\n",
    "- Test set generation is a critical part of a ML project\n",
    "  - but it is often neglected, which incurs bad even useless ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. shuffle then split data\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "  shuffled_indices = np.random.permutation(len(data))\n",
    "  test_set_size = int(len(data) * test_ratio)\n",
    "  test_indices = shuffled_indices[:test_set_size]\n",
    "  train_indices = shuffled_indices[test_set_size:]\n",
    "  return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issues: \n",
    "  - the random shuffling will generate different training set and test set every time\n",
    "  - overtime, the ML model will see the whole dataset\n",
    "- Tentative solutions:\n",
    "  1. save the test set on the first run and then load it in subsequent runs\n",
    "  2. fix the random number generator‚Äôs seed so that \n",
    "     - it always generates the same shuffled indices \n",
    "  - Problem: both these solutions will break the next time for an updated dataset\n",
    "- Common solution:\n",
    "  - use each instance‚Äôs identifier to decide whether or not it should go in the test set\n",
    "    - assuming instances have unique and immutable identifiers\n",
    "    - here, these identifiers are like container ids\n",
    "  - make these identifiers comparable such as hashing them\n",
    "    - then let the test set contains instances whose hashes no larger than 20% of the maximum hash value\n",
    "    - This ensures that the test set will remain consistent across multiple runs\n",
    "      - even the dataset is updated\n",
    "- These purely random sampling methods are generally fine for dataset large enough\n",
    "  - i.e. number of rows ‚â´ number of features\n",
    "  - otherwise, a significant sampling bias could be introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. use hash to ensures that \n",
    "# the test set will remain consistent across multiple runs\n",
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    " \n",
    "# The housing dataset does not have an identifier column. \n",
    "#  2.1 A simplest solution is to use the row index as the ID  \n",
    "# But this requires that \n",
    "#   new data gets appended to the end of the dataset \n",
    "#   and that no row ever gets deleted\n",
    "housing_with_id = housing.reset_index()  # adds an `index` column\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")  \n",
    "\n",
    "#  2.2 Another solution is \n",
    "#   combining district‚Äôs latitude and longitude into an ID like\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways\n",
    "- `train_test_split()` is the simplest function among them\n",
    "  - similar to the house-made `shuffle_and_split_data()` above\n",
    "  - but with a couple of additional features\n",
    "    - it has  a `random_state` parameter for setting the random generator seed\n",
    "    - it can split multiple datasets of the same size on the same indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. split dataset with train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the split randomness looks uniform and acceptable\n",
    "test_set[\"total_bedrooms\"].isnull().sum()/housing[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling\n",
    "- divides the population into homogeneous subgroups called strata\n",
    "- then samples the right number of instances from each stratum\n",
    "- guarantees that the test set is representative of the overall population\n",
    "- In a dataset, \n",
    "  - it is important to have a sufficient number of instances for each stratum\n",
    "  - otherwise, the estimate of a stratum‚Äôs importance may be biased\n",
    "- `sklearn.model_selection` package provides a number of splitter classes\n",
    "- Each splitter has a `split()` method that returns an iterator \n",
    "  - over different training/test splits of the same data\n",
    "  - it yields the training and test indices, not the data itself\n",
    "  - this is very useful for cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling\n",
    "# 1. the US population is 51.1% females and 48.9% males, \n",
    "#   so a well-conducted survey in the US would try to maintain \n",
    "#   this ratio in the sample: 511 females and 489 males\n",
    "\n",
    "# 1.1 the probability of getting a biased sample with \n",
    "#   <48.5% female or\n",
    "#   >53.5% female\n",
    "# The `cdf()` method of the binomial distribution gives us \n",
    "# the probability of `the number of females ‚â§ the given value`\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)\n",
    "proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)\n",
    "print(proba_too_small + proba_too_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Obtain the same result by simulation\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Stratify the median incomes in the California house dataset\n",
    "#   create an income category attribute with five categories \n",
    "#   labeled from 1 to 5\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 visualize the income categories\n",
    "\n",
    "axes2 = housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
    "axes2.set_xlabel(\"Income category\")\n",
    "axes2.set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generates 10 different splits of the California house dataset\n",
    "#    stratified on the income categories\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "strat_splits = []\n",
    "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set_n = housing.iloc[train_index]\n",
    "    strat_test_set_n = housing.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 use the last split\n",
    "strat_train_set, strat_test_set = strat_splits[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 a short way to get a single stratified split\n",
    "#   with `train_test_split`\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 check if the stratified split worked as expected\n",
    "#   check the similarity of this figure with the previous one\n",
    "\n",
    "axes3 = (strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)).sort_index().plot.bar(rot=0, grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 compares the income category proportions in  \n",
    "#   the overall dataset, \n",
    "#   the test set generated with stratified sampling, \n",
    "#   the test set generated using purely random sampling\n",
    "\n",
    "def income_cat_proportions(data):\n",
    "    return data[\"income_cat\"].value_counts() / len(data)\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall %\": income_cat_proportions(housing),\n",
    "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
    "    \"Random %\": income_cat_proportions(test_set),\n",
    "}).sort_index()\n",
    "compare_props.index.name = \"Income Category\"\n",
    "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
    "                                   compare_props[\"Overall %\"] - 1)\n",
    "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
    "                                  compare_props[\"Overall %\"] - 1)\n",
    "(compare_props * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows\n",
    "- the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, \n",
    "- whereas the test set generated using purely random sampling is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 the income_cat column can be dropped if it is not used again\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "  set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Visualize the Data to Gain Insights\n",
    "- put the test set aside and explore the training set only\n",
    "- better work on a copy of the training set \n",
    "  - since various transformations will be experimented\n",
    "- sample an exploration set if the training set is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make a copy of the original training set since it is small\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Geographical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 the data point distribution looks like California\n",
    "\n",
    "fig4, axes4 = plt.subplots(figsize=(6,4))\n",
    "housing.plot(ax=axes4, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 visualize data point density with partially transparency\n",
    "# What are those areas of high density?\n",
    "\n",
    "fig5, axes5 = plt.subplots(figsize=(6,4))\n",
    "housing.plot(ax=axes5, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. visualize \n",
    "#   housing prices with pseudo-colors\n",
    "#   district's population with circle sizes\n",
    "# It shows the housing prices are very much related to \n",
    "#   the location and \n",
    "#   the population density\n",
    "\n",
    "fig6, axes6 = plt.subplots(figsize=(10,7))\n",
    "housing.plot(ax=axes6, kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. visualize house price and district's population on the map of California\n",
    "filename = \"../datasets/california.png\"\n",
    "\n",
    "housing_renamed = housing.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (·¥ús·¥Ö)\"})\n",
    "\n",
    "fig7, axes7 = plt.subplots(figsize=(10,7))\n",
    "housing_renamed.plot(ax=axes7,\n",
    "             kind=\"scatter\", x=\"Longitude\", y=\"Latitude\",\n",
    "             s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "             c=\"Median house value (·¥ús·¥Ö)\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True, sharex=False)\n",
    "\n",
    "california_img = plt.imread( filename)\n",
    "axis = (-124.55, -113.95, 32.45, 42.05)\n",
    "axes7.axis(axis)\n",
    "axes7.imshow(california_img, extent=axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for [Correlations](https://en.wikipedia.org/wiki/Correlation) between features\n",
    "- The correlation coefficient only measures linear correlations, such as\n",
    "  - x goes up, y generally goes up/down\n",
    "  - this has nothing to do with the slope\n",
    "- It may completely miss out on nonlinear relationships\n",
    "- Purposes\n",
    "  - identify and clean outliers\n",
    "  - identify and transform skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute the standard correlation coefficient between every pair of attributes\n",
    "#     also called Pearson‚Äôs r\n",
    "corr_matrix = housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 find out how much each attribute correlates with the median house value\n",
    "# Explain\n",
    "#   - positive correlation, negative correlation and no correlation\n",
    "\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. check for correlation between attributes \n",
    "#     using the Pandas scatter_matrix() function\n",
    "# It plots every numerical attribute against every other numerical attribute. \n",
    "#    - Since there are now 11 numerical attributes, \n",
    "#       - you would get 11¬≤ = 121 plots\n",
    "#    - we may choose a few attributes that seem \n",
    "#       - most correlated with the median housing value\n",
    "# Note: Pandas displays a histogram of each attribute on the main diagonal\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "\n",
    "fig8, axes8 = plt.subplots(4,4, figsize=(12, 8))\n",
    "scatter_matrix(housing[attributes], ax=axes8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there is a strong positive correlation between `median_income` and `median_house_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. zoom in the correlation between `median_income` and `median_house_value`\n",
    "# there are several horizontal lines in the figure\n",
    "# the most obvious horizontal line is at $500,000,\n",
    "#   which is the price cap\n",
    "# It is better remove the corresponding districts from the data set\n",
    "\n",
    "fig9, axes9 = plt.subplots(figsize=(6, 4))\n",
    "housing.plot(ax=axes9, kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1, grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Attribute Combinations\n",
    "- attribute combinations could be more meaningful than their attributes alone\n",
    "- such as,\n",
    "  - the number of rooms per household\n",
    "  - the population per household\n",
    "  - the ratio of the number of bedrooms to the number of rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Attribute combination\n",
    "\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. verify the correlations between the attribute combinations and the target\n",
    "# Are they more correlated to the target than their component attributes?\n",
    "#  - bedrooms_ratio vs. total_rooms or total_bedrooms\n",
    "#  - rooms_per_house vs. total_rooms or households\n",
    "#  - people_per_house vs. population or households\n",
    "\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. revert to a clean training set\n",
    "#   - separate the predictors and the labels\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # no change in place\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "- Three options to an attributes with some missing values\n",
    "  1. drop the missing values with `dropna()`\n",
    "  2. remove the whole attribute with `drop()`\n",
    "  3. fill the missing values with some values such as zero, the mean, the median, etc.\n",
    "     - with `fillna()`\n",
    "     - this is called `imputation`\n",
    "- use `SimpleImputer` for option 3 instead of `fillna()`, because `SimpleImputer` has more features\n",
    "  - it will store the median value of each feature\n",
    "  - it is possible to impute missing values on \n",
    "    - the training set, the validation set, \n",
    "    - the test set, and any new data fed to the model\n",
    "- other more powerful imputers in sklearn.impute package, to replace missing values\n",
    "  - `KNNImputer` uses the mean of the k-nearest neighbors‚Äô values of that feature\n",
    "  - `IterativeImputer` uses predicted missing values with a regression model per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rows that originally contained a NaN value\n",
    "null_rows_idx = housing.isnull().any(axis=1)\n",
    "housing.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: drop the missing values\n",
    "\n",
    "housing_option1 = housing.copy()\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  \n",
    "housing_option1.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: remove the whole attribute, i.e. column\n",
    "housing_option2 = housing.copy()\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True) \n",
    "housing_option2.loc[null_rows_idx].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3: fill the missing values with some values such as zero, the mean, the median, etc.\n",
    "housing_option3 = housing.copy()\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  \n",
    "housing_option3.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create an imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\") # replace NaNs with the median of the attribute\n",
    "# other strategies: mean, most_frequent, constant, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 strategy=\"median\" only works ono numerical attributes\n",
    "# create a copy of the data with only the numerical attributes\n",
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 fit the imputer instance to the training data\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 the imputer simply computed the median of each attribute and\n",
    "#  stored the result in its statistics_ instance variable.\n",
    "#  It is safer to apply the imputer to all the numerical attributes\n",
    "#  preparing for any missing values in new data.\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the above results with\n",
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 use this ‚Äútrained‚Äù imputer to transform the training set\n",
    "#   by replacing missing values with the learned medians\n",
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 X is a NumPy array, \n",
    "# it can be wrapped in a DataFrame and \n",
    "#   recover the column names and index from housing_num.\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.loc[null_rows_idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 Predict outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 Drop outliers\n",
    "# uncomment and run these codes if you want to drop outliers\n",
    "#housing = housing.iloc[outlier_pred == 1]\n",
    "#housing_labels = housing_labels.iloc[outlier_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Text and Categorical Attributes\n",
    "- The values of categorical attributes are limited\n",
    "- They can be encoded in integers with `OrdinalEncoder` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. There is only one text attribute `ocean_proximity` in housing\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. `ocean_proximity` has just 5 different values\n",
    "#  each of which represents a category, so it is a categorical attribute.\n",
    "# Convert these categories from text to numbers \n",
    "#   since most machine learning algorithms prefer to work with numbers.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded[:5], housing_cat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issue of `OrdinalEncoder`: ML algorithms will assume that two nearby values are more similar than two distant values\n",
    "  - fine with ordered categories such as ‚Äúbad‚Äù, ‚Äúaverage‚Äù, ‚Äúgood‚Äù, and ‚Äúexcellent‚Äù\n",
    "  - but bad with unordered categories such USA state names\n",
    "- `one-hot encoding` can fix this issue\n",
    "  - implemented in sklearn class `OneHotEncoder`\n",
    "  - it encodes the categories with a binary string\n",
    "    - the length of the binary string equals the number of categories, \n",
    "    - each code has only one bit that is set to 1 for the coded category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. encode `ocean_proximity` categories with `OneHotEncoder`\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the `OneHotEncoder` class returns a sparse array,\n",
    "# because it is  a very efficient representation for matrices that contain mostly zeros,\n",
    "#  it only stores the nonzero values and their positions.\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert it to a dense array if needed by calling the `toarray()` method:\n",
    "housing_cat_1hot.toarray()[:5] # compare it with housing_cat_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or specify that `OneHotEncoder` class returns a dense array\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issue of `OneHotEncoder`\n",
    "  - results in a large number of input features if the categorical attribute has a large number of possible categories\n",
    "  - i.e. the binary code is very long\n",
    "  - This may slow down training and degrade performance\n",
    "- Possible solutions:\n",
    "  - replace the categorical input with useful numerical features related to the categories\n",
    "    - e.x.  replace the `ocean_proximity` feature with the distance to the ocean\n",
    "  - replace each category with a learnable, low-dimensional vector called an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Transformation\n",
    "- Feature scaling is one of the most important transformations on data\n",
    "  - Most ML algorithms don't perform well on numerical attributes with very different scales\n",
    "  - they bias toward the features with large scales\n",
    "- Two common feature scaling techniques:\n",
    "  -  `min-max scaling` implemented in sklearn class `MinMaxScaler`\n",
    "     - $\\displaystyle v'=\\frac{v-v_{min}}{v_{max}-v_{min}}$\n",
    "  -  `standardization` in `StandardScaler`\n",
    "     - $\\displaystyle v'=\\frac{v-v_{mean}}{œÉ_v}$\n",
    "     - $œÉ_v$ is the standard deviation\n",
    "- standardization \n",
    "  - does not restrict values to a specific range\n",
    "  - is much less affected by outliers\n",
    "  - will break a sparse matrix unless set its `with_mean` hyperparameter to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. `min-max` scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. `standardization` scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Long-tail distribution is generally transformed to be roughly symmetrical for better ML training\n",
    "  - but both min-max scaling and standardization will squash most values into a small range\n",
    "- Ways to shrink the heavy tail before scaling\n",
    "  - replace the feature with its square root\n",
    "    - or raise the feature to a power between 0 and 1\n",
    "  - replace the feature under a [power law distribution](https://en.wikipedia.org/wiki/Power_law) by its logarithm\n",
    "  - bucket the feature\n",
    "    - chop its distribution into roughly equal-sized buckets, \n",
    "    - and replace each feature value with the index of the bucket it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. replace the feature under a power law distribution\n",
    "# by its logarithm\n",
    "\n",
    "figa, axesa = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "housing[\"population\"].hist(ax=axesa[0], bins=50)\n",
    "housing[\"population\"].apply(np.log).hist(ax=axesa[1], bins=50)\n",
    "axesa[0].set(xlabel=\"Population\", title='A long tail')\n",
    "axesa[1].set(xlabel=\"Log of population\", title='Close to normal distribution')\n",
    "axesa[0].set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. replace each value with its percentile\n",
    "# Bucketizing with equal-sized buckets results in \n",
    "# a feature with an almost uniform distribution.\n",
    "\n",
    "percentiles = [np.percentile(housing[\"median_income\"], p)\n",
    "               for p in range(1, 100)]\n",
    "flattened_median_income = pd.cut(housing[\"median_income\"],\n",
    "                                 bins=[-np.inf] + percentiles + [np.inf],\n",
    "                                 labels=range(1, 100 + 1))\n",
    "figb, axesb = plt.subplots(1, 1, figsize=(5, 3))\n",
    "flattened_median_income.hist(bins=50, ax=axesb)\n",
    "axesb.set_xlabel(\"Median income percentile\")\n",
    "axesb.set_ylabel(\"Number of districts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a model and train it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Present your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [sklearn user guide](https://scikit-learn.org/stable/user_guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
