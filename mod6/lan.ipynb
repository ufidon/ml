{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod6/lan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod6/lan.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "Natural Language Processing (NLP) with RNNs and Attention\n",
    "---\n",
    "_homl3 ch16_\n",
    "\n",
    "NPL models in ascending order of capability\n",
    "- `Character RNN` (char-RNN) predicts the next character in a sentence\n",
    "  - able to generate some original text\n",
    "- `Stateless RNN` learns on `random portions` of text at each iteration \n",
    "  - without any information on the rest of the text\n",
    "- `Stateful RNN` preserves the hidden state between training iterations and continues reading where it left off\n",
    "  - able to learn longer patterns\n",
    "- `Sentiment RNN` extracts movie raters' feeling about movies from their reviews\n",
    "- `Neural machine translation (NMT)` translates English to Spanish \n",
    "  - based on an encoder‚Äìdecoder architecture\n",
    "- NMT boosted with `attention mechanism`\n",
    "  - learns to select `the part of the inputs` that the rest of the model should `focus` on at each time step\n",
    "- `Transformer` is a very successful attention-only architecture\n",
    "  - used by GPT and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab: Go to Runtime > Change runtime and select a GPU hardware\n",
    "# Kaggle: Go to Settings > Accelerator and select GPU\n",
    "# ‚ö†Ô∏è It may take more than one day to run the whole notebook without GPU\n",
    "import sys, os, math, copy\n",
    "from pathlib import Path\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds\n",
    "import tensorflow as tf, tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Demo of char-RNN\n",
    "---\n",
    "Open the [link](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) and explore examples generated by a `character RNN` from [Andrej Karpathy's char-rnn project](https://github.com/karpathy/char-rnn):\n",
    "- Shakespeare\n",
    "- Linux source code\n",
    "- Baby names\n",
    "\n",
    "Next, let's build a char-RNN step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating the Training Dataset\n",
    "# 1) download the Shakespeare data from Andrej Karpathy's \n",
    "#     [char-rnn project](https://github.com/karpathy/char-rnn/)\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) print the first few lines of Shakespeare\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Shakespeare has only 39 distinct characters (after converting to lower case)\n",
    "shakespeare_charset = \"\".join(sorted(set(shakespeare_text.lower())))\n",
    "len(shakespeare_charset), shakespeare_charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) encode Shakespeare text with character-level encoding \n",
    "#     rather than the default word-level encoding\n",
    "#     and convert the text to lowercase\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", \n",
    "                                                   standardize=\"lower\") \n",
    "text_vec_layer.adapt([shakespeare_text]) # create the vocabulary, here the set of characters\n",
    "encoded = text_vec_layer([shakespeare_text])[0] # encode Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(encoded.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each character is now mapped to an integer, starting at 2\n",
    "# The `TextVectorization layer` reserved \n",
    "#   the value 0 for padding tokens, \n",
    "#   1 for unknown characters. \n",
    "(shakespeare_text[:10]).lower(), encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overloading the meaning of tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "encoded -= 2  \n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shakespeare_text[:10]).lower(), encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5)  a small utility function used to \n",
    "#     (p1) turn this long sequence `encoded` into a dataset of windows \n",
    "#     for training a sequence-to-sequence RNN\n",
    "#    The targets will be similar to the inputs,\n",
    "#     but shifted by one character into the right\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) split the dataset into training set, validation set and test set\n",
    "#  the RNN will not be able to learn any pattern longer than `length`, \n",
    "#   so don‚Äôt make it too small\n",
    "length = 100\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Building and Training the Char-RNN Model\n",
    "# 1) build the Char-RNN Model with a GRU layer with 128 units\n",
    "model = tf.keras.Sequential([\n",
    "  # number of input dimensions is the number of distinct character IDs, \n",
    "  # and the number of output dimensions is a tunable hyperparameter\n",
    "  tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "  # the inputs of the Embedding layer: \n",
    "  #     2D tensors of shape [batch size, window length], \n",
    "  # the output of the Embedding layer: \n",
    "  #     a 3D tensor of shape [batch size, window length, embedding size]\n",
    "  \n",
    "  tf.keras.layers.GRU(128, return_sequences=True),\n",
    "  \n",
    "  # the output layer must have 39 units (n_tokens) \n",
    "  #   because there are 39 distinct characters in the text, \n",
    "  #   and we want to output a probability for each possible character at each time step. \n",
    "  #   The 39 output probabilities should sum up to 1 at each time step\n",
    "  tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train the Char-RNN Model\n",
    "# setup a callback\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Without a GPU, it may take over 24 hours.\n",
    "# skip the next two code cells\n",
    "if \"google.colab\" in sys.modules:\n",
    "  physical_devices = tf.config.list_physical_devices('GPU')\n",
    "  if len(physical_devices) == 0:\n",
    "    print(\"no gpu\")\n",
    "  else:\n",
    "    print('with gpu')\n",
    "    history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                        callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) wrap text preprocessing and the model together\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a pretrained model\n",
    "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
    "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\n",
    "model_path = Path(path).with_name(\"shakespeare_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) make a prediction\n",
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate fake Shakespearean text using `greedy decoding`\n",
    "#   a) feed the char-RNN model some text\n",
    "#       make it predict the most likely next letter ‚Ñì‚Çô\n",
    "#   b) add ‚Ñì‚Çô to the end of the text, then feed the extended text \n",
    "#       to the model to guess the next letter, and so on\n",
    "# shortcoming: this often leads to the same words being repeated over and over again\n",
    "# solution:  sample the next character randomly with a probability \n",
    "#             equal to the estimated probability\n",
    "\n",
    "# 1) demo: draw samples randomly based on logits distribution\n",
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2) helper function `next_char` picks the next character \n",
    "#     by `simulated annealing` \n",
    "# Lower temperature favors high-probability characters\n",
    "# higher temperature gives all characters an equal probability\n",
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) helper function `extend_text` repeatedly calls next_char() \n",
    "#     to get the next character and append it to the given text\n",
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) generate some text with different temperatures\n",
    "# low temperature like a person with a rigidified mind\n",
    "print(extend_text(\"\\nTo be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# middle temperature like a normal person\n",
    "print(extend_text(\"\\nTo be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high temperature like a man with a fever\n",
    "print(extend_text(\"\\nTo be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement\n",
    "---\n",
    "- other character sampling techniques\n",
    "  - `nucleus sampling` samples only from \n",
    "    - the top k characters, \n",
    "    - or the smallest set of top characters whose total probability exceeds some threshold\n",
    "  - ` beam search`\n",
    "- further model tuning\n",
    "  - use more GRU layers and more neurons per layer\n",
    "  - make the window larger\n",
    "  - train for longer and add some regularization if needed, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stateful RNN\n",
    "---\n",
    "- for `stateless RNNs`, at each training iteration the model \n",
    "  - starts with a hidden state full of zeros\n",
    "  - then it updates this state at each time step, \n",
    "  - and after the last time step, it throws it away as it is not needed anymore\n",
    "- for `stateful RNNs`, the model \n",
    "  - preserves this final state after processing a training batch \n",
    "  - and use it as the initial state for the next training batch\n",
    "  - ‚à¥ it can learn long-term patterns despite only backpropagating through short sequences\n",
    "- A stateful RNN only makes sense if `each input sequence in a batch` starts exactly where the `corresponding sequence in the previous batch left off`\n",
    "  - ‚à¥ (p2) its input sequences must be `sequential and nonoverlapping`\n",
    "  - rather than the `shuffled and overlapping` sequences used to train stateless RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prepare a dataset for a stateful RNN\n",
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n",
    "                                                 length)\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of `to_dataset_for_stateful_rnn`\n",
    "list(to_dataset_for_stateful_rnn(tf.range(10), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) prepare dataset for more than one window per batch\n",
    "#  use `to_batched_dataset_for_stateful_rnn()` function instead of \n",
    "# `to_dataset_for_stateful_rnn()`\n",
    "def to_non_overlapping_windows(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
    "\n",
    "def to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n",
    "    parts = np.array_split(sequence, batch_size)\n",
    "    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts)\n",
    "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create the stateful RNN\n",
    "# [stateful=True](https://keras.io/2.15/api/layers/recurrent_layers/rnn/)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of each epoch, we need to reset the states before \n",
    "# we go back to the beginning of the text\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. compile and train the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
    "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. A stateless copy is needed to use the stateful model with different batch sizes\n",
    "stateless_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. build the model and set its weights\n",
    "stateless_model.build(tf.TensorShape([None, None]))\n",
    "stateless_model.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    stateless_model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"to be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean temporary files\n",
    "!rm -rf ./my_shakespeare_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis\n",
    "- one type of text classification based on `word-level` models\n",
    "  - instead of `character-level` models like char-RNN\n",
    "- predicts reviewers' feelings about a movie such as negative (0) or positive (1)\n",
    "  - based on their review texts on this movie\n",
    "- needs to handle sequences of variable lengths using masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load and split the IMDb dataset\n",
    "# which consists of 50,000 movie reviews in English \n",
    "# (25,000 for training, 25,000 for testing) extracted from \n",
    "# the famous Internet Movie Database, along with a simple binary target \n",
    "# for each review indicating whether it is negative (0) or positive (1)\n",
    "\n",
    "# 1) load and split the IMDb dataset\n",
    "# 90% of the training set for training, the remaining 10% for validation:\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) show a few reviews\n",
    "# a) some reviews are easy to classify since they contain sentimental words such as\n",
    "#    `terrible`, `wonderful`, etc.\n",
    "# b) some reviews are challenging since they may contain turns such\n",
    "#     starting positively then turning negative, etc.\n",
    "#\n",
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine words with subword tokenization\n",
    "---\n",
    "- Keras layer `TextVectorization` can be used to identify word boundaries by spaces\n",
    "  - it may not work well in some languages such as \n",
    "    - Chinese, Japanese and Korean which do not use spaces between words\n",
    "    - Vietnamese and some English words uses spaces even within words: San Francisco\n",
    "    - German and some English words often attach multiple words together without spaces: ILoveDeepLearning\n",
    "  - solutions by `subword tokenization`\n",
    "    - [Byte pair encoding (BPE)](https://homl.info/wordpiece) splits the whole training set into individual characters (including spaces)\n",
    "      - then repeatedly merges the most frequent adjacent pairs until the vocabulary reaches the desired size\n",
    "    - [Subword regularization](https://github.com/google/sentencepiece) improves accuracy and robustness by introducing some randomness in tokenization during training\n",
    "      - ex. \"New England\", \"New\"+\"England\", or \"New\"+\"Eng\"+\"land\"\n",
    "    - The [Tokenizers library by Hugging Face](https://huggingface.co/docs/tokenizers/index) implements a wide range of extremely fast tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenize IMDb reviews\n",
    "# a) limit the vocabulary to 1,000 tokens\n",
    "#     including the most frequent 998 words, a padding token and a token for unknown words\n",
    "#     since it‚Äôs unlikely that very rare words will be important for this task\n",
    "# b) this limit reduces the number of parameters the model needs to learn\n",
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create and train a model\n",
    "# the model will probably not learn anything because we didn't mask the padding tokens\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy of the previous model remains close to 50%\n",
    "  - i.e. no better than random guess\n",
    "- Reasons\n",
    "  - The reviews have `different lengths`, but the TextVectorization layer \n",
    "    - converts them to sequences of token IDs\n",
    "    - then pads the shorter sequences using the padding token (with ID 0) to make them as long as `the longest sequence in the batch`\n",
    "  - As a result, most sequences end with `many padding tokens`‚Äîoften dozens or even hundreds of them\n",
    "    - a GRU layer with only short-term memory forgets what the review was about after it goes through many padding tokens\n",
    "- Solutions\n",
    "  - ‚ù∂ feed the model with batches of `equal-length` sentences\n",
    "    - which also speeds up training\n",
    "  - ‚ù∑ make the RNN ignore the padding tokens using `masking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking\n",
    "---\n",
    "- enabled by simply adding `mask_zero=True` when creating the Embedding layer\n",
    "  - then the padding tokens (whose ID is 0) will be ignored by all downstream layers\n",
    "    - when a recurrent layer encounters a masked time step\n",
    "      - it simply copies the output from the previous time step\n",
    "- supported by many Keras layers such as\n",
    "  - SimpleRNN, GRU, LSTM, Bidirectional, Dense, TimeDistributed, Add, etc.\n",
    "- If a layer‚Äôs `supports_masking=True` then the mask is automatically propagated to the next layer\n",
    "  - It keeps propagating this way for as long as the layers have `supports_masking=True`\n",
    "- A recurrent layer‚Äôs `supports_‚Äãmask‚Å†ing` attribute is `True` when `return_sequences=True`\n",
    "  - but it‚Äôs `False` when `return_‚Äãsequen‚Å†ces=False` so it will not propagate the mask any further\n",
    "- If the mask propagates all the way to the output then it gets applied to the losses as well\n",
    "  - so the masked time steps will not contribute to the loss (their loss will be 0)\n",
    "  - This assumes that the model outputs sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Enable masking \n",
    "# 1) by turning `mask_zero=True` in the Embedding layer\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) or by manual masking\n",
    "# explicitly compute the mask and pass it to the appropriate layers,\n",
    "# using either the functional API or the subclassing API\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "\n",
    "#  add a bit of dropout since the previous model was overfitting slightly\n",
    "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles and trains the model as usual\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) One last approach to masking \n",
    "# feed the model with ragged tensors by setting `ragged=True` \n",
    "# when creating the TextVectorization layer, \n",
    "# so that the input sequences are represented as ragged tensors\n",
    "\n",
    "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size, ragged=True)\n",
    "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this ragged tensor representation \n",
    "# with the regular tensor representation, which uses padding token\n",
    "text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and train the model with ragged tensors\n",
    "\n",
    "embed_size = 128\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer_ragged,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing Pretrained Embeddings and Language Models\n",
    "---\n",
    "- many words are used context-freely\n",
    "  - such as `awesome` and `amazing` have positive meaning in various contexts\n",
    "  - so pretrained embeddings are widely reused such as\n",
    "    - Googles [Word2vec embeddings](https://homl.info/word2vec)\n",
    "    - Stanford's [GloVe embeddings](https://homl.info/glove)\n",
    "    - Facebook's [FastText embeddings](https://fasttext.cc/)\n",
    "- however, there are also many words whose meanings depend on their contexts\n",
    "  - such as `right` in `left and right` and `right and wrong`\n",
    "    - while it has just a single representation in word embeddings\n",
    "  - addressed by [Embeddings from Language Models (ELMo)](https://homl.info/elmo)\n",
    "    - ELMo are contextualized word embeddings learned from the internal states of a deep bidirectional language model\n",
    "    - they allow reusing part of a pretrained language model \n",
    "      - instead of just using pretrained embeddings in your model\n",
    "- [Universal Language Model Fine-Tuning (ULMFiT)](https://homl.info/ulmfit) demonstrated the effectiveness of unsupervised pretraining for NLP tasks\n",
    "  - ex. the [Universal Sentence Encoder](https://homl.info/139) based on the transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the `Universal Sentence Encoder` model from TensorFlow Hub\n",
    "# This model is quite large, close to 1 GB in size\n",
    "#   so it may take a while to download. \n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# By default, TensorFlow Hub models are saved to a temporary directory, \n",
    "# and they get downloaded again and again every time you run your program.\n",
    "if \"google.colab\" in sys.modules:\n",
    "    pass\n",
    "else:\n",
    "    os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
    "    \n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                   trainable=True, dtype=tf.string, input_shape=[]),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Encoder‚ÄìDecoder Network for Neural Machine Translation\n",
    "- (p3) A simple [NMT model](https://homl.info/103) that translates English sentences to Spanish\n",
    "  - English sentences are fed as inputs to the encoder\n",
    "  - The decoder outputs the Spanish translations\n",
    "    - also uses the Spanish translations as inputs during training but `shifted back` by one step\n",
    "      - i.e. the word output at the `previous step`\n",
    "      - This is called `teacher forcing`\n",
    "        - a technique that significantly speeds up training and improves the model‚Äôs  performance\n",
    "  - For the very first word\n",
    "    - the decoder is given the `start-of-sequence (SOS)` token \n",
    "    - the decoder is expected to end the sentence with an `end-of-sequence (EOS)` token\n",
    "  - Each word is initially represented by its ID\n",
    "  - Next, an Embedding layer returns the word embedding\n",
    "    - These word embeddings are then fed to the encoder and the decoder\n",
    "  - At each step the decoder outputs a score for each word in the output vocabulary (i.e., Spanish)\n",
    "    - then the `softmax` activation function turns these scores into probabilities\n",
    "- At inference time after training\n",
    "  - (p4) you will not have the target sentence to feed to the decoder\n",
    "  - Instead, you need to feed it the word that it has just output at the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the dataset\n",
    "# 1) download a dataset of English/Spanish sentence pairs\n",
    "# Each line contains an English sentence and the corresponding Spanish translation, \n",
    "#   separated by a tab\n",
    "\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) tidy the dataset\n",
    "#  a) remove the Spanish characters ‚Äú¬°‚Äù and ‚Äú¬ø‚Äù, \n",
    "#     which the TextVectorization layer doesn‚Äôt handle\n",
    "text = text.replace(\"¬°\", \"\").replace(\"¬ø\", \"\")\n",
    "\n",
    "#  b) parse the sentence pairs and shuffle them. \n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "#  c) Finally split them into two separate lists, one per language\n",
    "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) peek at the first three sentence pairs:\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create two TextVectorization layers\n",
    "# one per language and adapt them to the text:\n",
    "\n",
    "# a) using a small value will speed up training especially for this small dataset\n",
    "# State-of-the-art translation models typically use \n",
    "#   a much larger vocabulary (e.g., 30,000), \n",
    "#   a much larger training set (gigabytes), \n",
    "#   and a much larger model (hundreds or even thousands of megabytes)\n",
    "#   ex:  the Opus-MT models by the University of Helsinki, \n",
    "#        or the M2M-100 model by Facebook\n",
    "vocab_size = 1000\n",
    "\n",
    "# b) all sentences in the dataset have a maximum of 50 words\n",
    "#   setting `output_sequence_length=50` causes that \n",
    "#     the input sequences automatically been padded with zeros \n",
    "#     until they are all 50 tokens long\n",
    "#   any sentences longer than 50 tokens in the training set will be cropped to 50 tokens\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "\n",
    "# c) ‚Äústartofseq‚Äù and ‚Äúendofseq‚Äù are added to each sentence as SOS and EOS tokens\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) inspect the first 10 tokens in the English vocabulary\n",
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) inspect the first 10 tokens in the Spanish vocabulary\n",
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. split the dataset\n",
    "# a) the first 100,000 sentence pairs for training, \n",
    "#     and the rest for validation\n",
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "\n",
    "# b) The decoder‚Äôs inputs are the Spanish sentences plus an SOS token prefix. \n",
    "#     The targets are the Spanish sentences plus an EOS suffix\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. build the model with `functional API` \n",
    "#     since the model is not sequential\n",
    "# a)  It requires two text inputs:\n",
    "#       one for the encoder and one for the decoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) encode these sentences using the TextVectorization layers\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "# c) followed by an Embedding layer for each language, \n",
    "#     with `mask_zero=True` to ensure masking is handled automatically\n",
    "#   `embed_size` is a tunable hyperparameter\n",
    "embed_size = 128\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) create the encoder and pass it the embedded inputs\n",
    "# a single LSTM layer is used here, but you could stack several of them.\n",
    "#   `return_state=True` allows us get a reference to the layer‚Äôs final state\n",
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "\n",
    "# `*encoder_state` groups the LSTM layer's \n",
    "#     short-term state and long-term state in a list\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) use this encoder_state as the initial state of the decoder:\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) pass the decoder‚Äôs outputs through a Dense layer \n",
    "#     with the softmax activation function \n",
    "#     to get the word probabilities for each step\n",
    "# ‚ö†Ô∏è for a large output vocabulary, outputting a probability \n",
    "#     for each and every possible word can be quite slow.\n",
    "# two ways to speedup \n",
    "# i) sampled softmax technique looks only at the logits output by the model \n",
    "#     for the correct word and for a random sample of incorrect words, then \n",
    "#     compute an approximation of the loss based only on these logits\n",
    "# ii) tie the weights of the output layer to \n",
    "#     the transpose of the decoder‚Äôs embedding matrix\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) Assemble, compile and train the model\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. use the model to translate\n",
    "# It is NOT as simple as calling model.predict(), \n",
    "#     because the decoder expects as input the word that\n",
    "#     was predicted at the previous time step\n",
    "# two ways:\n",
    "# i) One way to do this is to write a custom memory cell that\n",
    "#     keeps track of the previous output and \n",
    "#     feeds it to the encoder at the next time step\n",
    "# ii) just call the model multiple times\n",
    "#     predicting one extra word at each round\n",
    "\n",
    "\n",
    "# 1) A utility function\n",
    "# it simply keeps predicting one word at a time, \n",
    "# gradually completing the translation,\n",
    "# and it stops once it reaches the EOS token\n",
    "\n",
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        X = np.array([sentence_en])  # encoder input \n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the model struggles with longer sentences:\n",
    "# The model can be improved a little bit by \n",
    "#   increase the training set size and \n",
    "#   adding more LSTM layers in both the encoder and the decoder\n",
    "\n",
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional RNNs\n",
    "---\n",
    "- A `regular` recurrent layer generates outputs based on only `past and present inputs` \n",
    "  - i.e. it cannot look into the future\n",
    "  - This makes sense when forecasting `time series` \n",
    "    - or in the `decoder` of a sequence-to-sequence (seq2seq) model\n",
    "  - but it is often preferable to look ahead at the next words before encoding a given word\n",
    "    - in `text classification` or in the `encoder` of a seq2seq model\n",
    "- (p5) A `bidirectional` recurrent layer runs two recurrent layers on the same inputs\n",
    "  - one reading the words from left to right \n",
    "  - the other reading them from right to left\n",
    "  - then combines their outputs at each time step \n",
    "    - typically by concatenating them\n",
    "  - ex. to properly encode the word ‚Äúright‚Äù in the phrases \n",
    "    - ‚Äúthe right arm‚Äù, ‚Äúthe right person‚Äù, and ‚Äúthe right to criticize‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  implement a bidirectional recurrent layer in Keras\n",
    "# 1) just wrap a regular recurrent layer in a `Bidirectional` layer\n",
    "#\n",
    "# i) The Bidirectional layer will create a clone of the GRU layer \n",
    "#   (but in the reverse direction), and it will run both and \n",
    "#   concatenate their outputs. So although the GRU layer has 10 units, \n",
    "#   the Bidirectional layer will output 20 values per time step\n",
    "\n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) state concatenation by type\n",
    "# the decoder‚Äôs LSTM layer expects just two states (short-term and long-term)\n",
    "# so we need to concatenate the two short-term states, \n",
    "#     and also concatenate the two long-term states\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) complete the model and train it\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) try a translation\n",
    "translate(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam Search\n",
    "---\n",
    "- A technique solves problematic translations such as \n",
    "  - ‚ÄúI like soccer‚Äù ‚Üí \"me gustan los jugadores\" instead of  ‚Äúme gusta el f√∫tbol\"\n",
    "  - if the training set contains many sentences like ‚ÄúI like cars‚Äù ‚Üí ‚Äúme gustan los autos‚Äù\n",
    "    - the model will learn the pattern `plural` but soccer is not a plural\n",
    "- keeps track of a short list of the `k most promising sentences` \n",
    "  - and at each decoder step it tries to extend them by one word \n",
    "    - calculating the extended sentences' popularity and again keeping only the k most likely sentences \n",
    "  - The parameter k is called the `beam width`\n",
    "- ex. (p6) suppose we use the model to translate the sentence ‚ÄúI like soccer‚Äù using beam search with a beam width of 3\n",
    "  - At the first decoder step, the model will output an estimated probability for each possible first word in the translated sentence\n",
    "    - Suppose the top three words are ‚Äúme‚Äù (75% estimated probability), ‚Äúa‚Äù (3%), and ‚Äúcomo‚Äù (1%)\n",
    "  - Next, we use the model to find the next word for each sentence\n",
    "    - For the first sentence (‚Äúme‚Äù), perhaps the model outputs a probability of 36% for the word ‚Äúgustan‚Äù, 32% for the word ‚Äúgusta‚Äù, 16% for the word ‚Äúencanta‚Äù, and so on\n",
    "      - Note that these are actually `conditional probabilities` given that the sentence starts with ‚Äúme‚Äù\n",
    "    - For the second sentence (‚Äúa‚Äù), the model might output a conditional probability of 50% for the word ‚Äúmi‚Äù, and so on.\n",
    "    - Assuming the vocabulary has 1,000 words, we will end up with 1,000 probabilities per sentence.\n",
    "  - Next, we compute the probabilities of each of the 3,000 two-word sentences we considered (3 √ó 1,000)\n",
    "    - We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes\n",
    "    - For example, the estimated probability of the sentence ‚Äúme‚Äù was 75%, while the estimated conditional probability of the word ‚Äúgustan‚Äù (given that the first word is ‚Äúme‚Äù) was 36%, so the estimated probability of the sentence ‚Äúme gustan‚Äù is 75% √ó 36% = 27%\n",
    "    - After computing the probabilities of all 3,000 two-word sentences, we `keep only the top 3`\n",
    "    - In this example they all start with the word ‚Äúme‚Äù: ‚Äúme gustan‚Äù (27%), ‚Äúme gusta‚Äù (24%), and ‚Äúme encanta‚Äù (12%). Right now, the sentence ‚Äúme gustan‚Äù is winning, but ‚Äúme gusta‚Äù has not been eliminated.\n",
    "  - Then we repeat the same process: \n",
    "    - we use the model to predict the next word in each of these three sentences\n",
    "    - and we compute the probabilities of all 3,000 three-word sentences we considered.\n",
    "    - Perhaps the top three are now ‚Äúme gustan los‚Äù (10%), ‚Äúme gusta el‚Äù (8%), and ‚Äúme gusta mucho‚Äù (2%). \n",
    "    - At the next step we may get ‚Äúme gusta el f√∫tbol‚Äù (6%), ‚Äúme gusta mucho el‚Äù (1%), and ‚Äúme gusta el deporte‚Äù (0.2%). \n",
    "    - Notice that ‚Äúme gustan‚Äù was eliminated, and the correct translation is now ahead. \n",
    "  - We boosted our encoder‚Äìdecoder model‚Äôs performance without any extra training, simply by  using it more wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. a basic implementation of beam search\n",
    "\n",
    "def beam_search(sentence_en, beam_width, verbose=False):\n",
    "    X = np.array([sentence_en])  # encoder input\n",
    "    X_dec = np.array([\"startofseq\"])  # decoder input\n",
    "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    top_translations = [  # list of best (log_proba, translation)\n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
    "    ]\n",
    "    \n",
    "    # extra code ‚Äì displays the top first words in verbose mode\n",
    "    if verbose:\n",
    "        print(\"Top first words:\", top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(\"endofseq\"):\n",
    "                candidates.append((log_proba, translation))\n",
    "                continue  # translation is finished, so don't try to extend it\n",
    "            X = np.array([sentence_en])  # encoder input\n",
    "            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba),\n",
    "                                   f\"{translation} {word}\"))\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # extra code ‚Äì displays the top translation so far in verbose mode\n",
    "        if verbose:\n",
    "            print(\"Top translations so far:\", top_translations)\n",
    "\n",
    "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
    "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. shows how the model making an error\n",
    "sentence_en = \"I love cats and dogs\"\n",
    "translate(sentence_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms\n",
    "- (p7) the game-changing innovation that addressed the limited short-term memory of RNNs\n",
    "  - by focusing its attention on important source words such as \"soccer\" for the destination words such as ‚Äúf√∫tbol‚Äù \n",
    "  - using `higher weights` for words with `more attention` in a weighted sum of all the encoder outputs\n",
    "  - in (p7.middle-left): ${\\displaystyle \\mathbf{h}_{(2)} = ‚àë_{i=0}^2 Œ±_{(3,i)} \\mathbf{\\hat{y}}_{(i)} }$\n",
    "  - ${ Œ±_{(t,i)} }$ is the weight of the ${ i^{th} }$ encoder output ${ \\hat{y}_{(i)} }$ at the ${ t^{th} }$ decoder time step\n",
    "  - these ${ Œ±_{(t,i)} }$ weights are generated by a small neural network called an `alignment model (or an attention layer)` (p7.right)\n",
    "    - which is trained jointly with the rest of the encoder‚Äìdecoder model\n",
    "  - the attention layer  \n",
    "    - starts with a Dense layer composed of a single neuron that processes each of the encoder‚Äôs outputs\n",
    "      - along with the decoder‚Äôs previous hidden state (e.g., ${\\mathbf{h}_{(2)}}$)\n",
    "    - outputs for each encoder output (e.g., ${e_{(3, 2)}}$) a `score (or energy)` \n",
    "      - that measures how well `each output is aligned with the decoder‚Äôs previous hidden state`\n",
    "        - the one that best aligns with the current state gets a high score\n",
    "      - Finally, all the scores go through a `softmax` layer to get a final weight for each encoder output (e.g., ${ Œ±_{(3,1)} }$)\n",
    "        - All the weights for a given decoder time step add up to 1\n",
    "  - Since it `concatenates` the encoder output with the decoder‚Äôs previous hidden state\n",
    "    - it is sometimes called `concatenative attention (or additive attention)`\n",
    "  - Another common attention mechanism is `multiplicative attention`\n",
    "    - simply computes the `dot product` of one of the encoder‚Äôs outputs and the decoder‚Äôs previous hidden state \n",
    "      - since the goal of the alignment model is to measure the `similarity` between these two vectors\n",
    "      - requires both vectors must have the same dimensionality\n",
    "    - uses the decoder‚Äôs hidden state at the current time step rather than at the previous time step (i.e. ${\\mathbf{h}_{(t)}}$ rather than ${\\mathbf{h}_{(t-1)}}$)\n",
    "    - then uses the output of the attention mechanism (noted ${\\mathbf{hÃÉ}_{(t)}}$ ) directly to compute the decoder‚Äôs predictions \n",
    "      - rather than using it to compute the decoder‚Äôs current hidden state\n",
    "  - A variant of the dot product mechanism where the encoder outputs first go through a fully connected layer (without a bias term) before the dot products are computed\n",
    "    - called `the ‚Äúgeneral‚Äù dot product approach`\n",
    "  - Both dot product variants performed better than concatenative attention\n",
    "  - These three attention mechanisms are summarized as\n",
    "    - ${\\displaystyle \\mathbf{hÃÉ}_{(t)} = ‚àë_{i}Œ±_{(t,i)}\\mathbf{y}_{(i)} }$ \n",
    "      - ${ Œ±_{(t,i)}=\\dfrac{e^{e_{(t,i)}}} {‚àë_k e^{e_{(t,i_k)}}} }$ \n",
    "      - ${ e_{(t,i)}=\\begin{cases} \\mathbf{h}_{(t)}^‚ä∫\\mathbf{y}_{(i)} & dot\\\\ \\mathbf{h}_{(t)}^‚ä∫ \\mathbf{W} \\mathbf{y}_{(i)} & general \\\\ \\mathbf{v}^‚ä∫\\tanh(\\mathbf{W}[\\mathbf{h}_{(t)};\\mathbf{y}_{(i)}]) & concat \\end{cases} }$\n",
    "- revolutionized NMT and deep learning in general\n",
    "  - allowing a significant improvement in the state of the art\n",
    "  - especially for long sentences (e.g., over 30 words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using attention mechanism\n",
    "# - tf.keras.layers.Attention layer for multiplicative attention\n",
    "# - tf.keras.layers.AdditiveAttention layer for additive attention\n",
    "# 1) `return_sequences=True` passes all the encoder‚Äôs outputs to the Attention layer\n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) create the attention layer and pass it the decoder‚Äôs states and the encoder‚Äôs outputs\n",
    "#  let‚Äôs use the decoder‚Äôs outputs instead of its states: \n",
    "#     in practice this works well too, and it‚Äôs much easier to code\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "#  Then we just pass the attention layer‚Äôs outputs directly to the output layer\n",
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) build and train the NMT model with attention mechanism\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) it can handle much longer sentences\n",
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Transformer](https://homl.info/transformer) model\n",
    "---\n",
    "- significantly improved the state-of-the-art in NMT \n",
    "  - without using any recurrent or convolutional layers,\n",
    "  - just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces)\n",
    "- benefits\n",
    "  - not recurrent\n",
    "    - can be trained in fewer steps\n",
    "    - easier to parallelize across multiple GPUs\n",
    "  - not suffer as much from the unstable gradients problems as RNNs\n",
    "  - can better capture long-range patterns than RNNs\n",
    "- The original 2017 transformer architecture is shown in (p8)\n",
    "  - the left part is the `encoder`\n",
    "    - it gradually transforms the source word representations until the representation perfectly captures the meaning of that word\n",
    "  - the right part is the `decoder`\n",
    "    - it gradually transform each word representation in the translated sentence into a word representation of the next word in the translation\n",
    "    - After going through the decoder, each word representation goes through a final Dense layer with a softmax activation function\n",
    "      - which will hopefully output a high probability for the correct next word and a low probability for all other words\n",
    "  - Each `embedding layer` outputs a 3D tensor of shape `[batch size, sequence length, embedding size]`\n",
    "    - the tensors are gradually transformed as they flow through the transformer\n",
    "      - but their shape remains the same\n",
    "- The big picture of the transformer for NMT\n",
    "  - during training you must feed the English sentences to the encoder \n",
    "    - the corresponding Spanish translations to the decoder with an extra SOS token inserted at the start of each sentence\n",
    "  - At inference time you must call the transformer multiple times to\n",
    "    - produce the translations one word at a time\n",
    "    - feed the partial translations to the decoder at each round\n",
    "  - The encoder‚Äôs `multi-head attention layer` updates each word representation by attending to all other words in the same sentence\n",
    "    - until each word‚Äôs representation perfectly captures the meaning of the word, in the  context of the sentence\n",
    "  - The decoder‚Äôs `masked multi-head attention layer` does the same thing\n",
    "    - but when it processes a word, it doesn‚Äôt attend to words located after it\n",
    "    - i.e. it‚Äôs a `causal` layer\n",
    "  - The decoder‚Äôs `upper multi-head attention layer` is where the decoder pays attention to the words in the English sentence\n",
    "    - This is called `cross-attention`\n",
    "  - After going through the decoder, \n",
    "    - each word representation goes through a final Dense layer with a `softmax` activation function\n",
    "    - which will hopefully output a high probability for the correct next word and a low probability for all other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encodings\n",
    "---\n",
    "- A dense vector that encodes the position of a word within a sentence\n",
    "  - the ${ i^{th} }$ positional encoding is added to the word embedding of the ${ i^{th} }$ word in the sentence\n",
    "    - using an Embedding layer and make it encode all the positions from 0 to the maximum sequence length in the batch\n",
    "    - then adding the result to the word embeddings by the rules of broadcasting\n",
    "    - this is a `trainable positional encodings`\n",
    "- There are `fixed positional encodings` such as the one below generating a positional encoding matrix ${ \\mathbf{P} }$ \n",
    "  - (p9) based on the sine and cosine functions at different frequencies\n",
    "    - ${\\displaystyle P_{(p,i)} = \\begin{cases} \\sin(\\dfrac{p}{10000^{i/d}}) & \\text{if } i \\text{ is even} \\\\ \\cos(\\dfrac{p}{10000^{(i-1)/d}}) & \\text{if } i \\text{ is odd} \\end{cases}  }$\n",
    "      - ${ P_{(p,i)} }$ is the ${ i^{th} }$ component of the encoding for the word located at the ${ p^{th} }$ position in the sentence\n",
    "  - this encoding can give the same performance as trainable positional encodings\n",
    "    - it can extend to arbitrarily long sentences without adding any parameters to the model\n",
    "    - but trainable positional encodings are preferred if there is a large amount of pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. add trainable positional encodings to the encoder and decoder inputs\n",
    "# Here we assume that the embeddings are represented as regular tensors, not ragged tensors\n",
    "# The encoder and the decoder share the same Embedding layer for the positional encodings, \n",
    "# since they usually have the same embedding size\n",
    "\n",
    "max_length = 50  # max length in the whole training set\n",
    "embed_size = 128\n",
    "\n",
    "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. fixed positional encodings based on sine/cosine\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        # 1) precompute the positional encoding matrix\n",
    "        p, i = np.meshgrid(np.arange(max_length),\n",
    "                           2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        # 2) enable propagation the input's automatic mask to the next layer\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the PositionalEncoding layer to add the positional encoding to the encoder‚Äôs inputs:\n",
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention\n",
    "---\n",
    "- based on the `scaled dot-production attention` layer \n",
    "  - ${\\displaystyle \\operatorname{Attention(\\mathbf{Q,K,V})} = \\operatorname{softmax}\\left( \\dfrac{\\mathbf{QK^‚ä∫}} {\\sqrt{d_{keys}}}  \\right)\\mathbf{V} }$\n",
    "    - ${ \\mathbf{Q} }$ is a matrix containing `one row per query`. \n",
    "      - Its shape is ${[n_{queries}, d_{keys}]}$\n",
    "    - ${ \\mathbf{K} }$ is a matrix containing one row per key\n",
    "      - Its shape is ${[n_{keys}, d_{keys}]}$\n",
    "    - ${ \\mathbf{K} }$ is a matrix containing one row per value\n",
    "      - Its shape is ${[n_{keys}, d_{values}]}$\n",
    "    - ${ \\mathbf{QK^‚ä∫} }$ contains one similarity score for each query/key pair\n",
    "    - The scaling factor ${ 1/\\sqrt{d_{keys}} }$ scales down the similarity scores to avoid saturating the softmax function\n",
    "      - it can be turned into a learnable parameter by setting `use_scale=True` when creating a `tf.keras.layers.Attention` layer\n",
    "    - It is possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity scores just before computing the softmax\n",
    "      - This is useful in the masked multi-head attention layer\n",
    "- has the architecture shown in (p10) which is just a bunch of scaled dot-product attention layers\n",
    "  - each preceded by a linear transformation of the values, keys, and queries\n",
    "    - a time-distributed dense layer with no activation function\n",
    "    - this allows the model to apply many different projections of the word representation into different `subspaces`\n",
    "      - each focusing on a `subset of the word‚Äôs characteristics` such as word type, tense, plural, etc.\n",
    "  - All the outputs are simply concatenated\n",
    "    - and they go through a final linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build a transformer\n",
    "# Keras includes a tf.keras.layers.MultiHeadAttention layer\n",
    "# 1) build the full encoder \n",
    "#     use a stack of two blocks (N = 2) \n",
    "#     since we don‚Äôt have a huge training set, \n",
    "\n",
    "N = 2  # instead of 6\n",
    "num_heads = 8\n",
    "\n",
    "# add a bit of dropout as well:\n",
    "dropout_rate = 0.1\n",
    "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) build the decoder\n",
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)\n",
    "\n",
    "encoder_outputs = Z  # let's save the encoder's final outputs\n",
    "Z = decoder_in  # the decoder starts with its own inputs\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) add the final output layer, \n",
    "# create the model, compile it, and train it\n",
    "\n",
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) try a translation\n",
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The burst of Transformer Models\n",
    "- started from the `ImageNet moment for NLP` in 2018\n",
    "  - with larger and larger transformer-based architectures trained on immense datasets\n",
    "- the [GPT paper](https://homl.info/gpt) demonstrated the effectiveness of unsupervised pretraining using a transformer-like architecture composed of a stack of 12 transformer modules using only `masked multi-head attention layers`\n",
    "- the [Google's BERT(Bidirectional Encoder Representations from Transformers) paper](https://homl.info/bert) also demonstrated the effectiveness of self-supervised pretraining on a large corpus, using a similar architecture to GPT but with `nonmasked multi-head attention layers` only, like in the original transformer‚Äôs encoder, in two pretraining tasks\n",
    "  - ‚ë† `Masked language model (MLM)`  is trained to predict the masked words\n",
    "  - ‚ë° `Next sentence prediction (NSP)` is trained to predict whether two sentences are consecutive or not\n",
    "    - not as important as was initially thought, so it was dropped in most later architectures\n",
    "- (p11) The BERT model is trained on MLM and NSP simultaneously on a very large corpus of text\n",
    "  - then fine-tuned on many different tasks, changing very little for each task\n",
    "- In 2019, the [GPT-2 paper](https://homl.info/gpt2) proposed a very similar architecture to GPT but with over 1.5 billion parameters\n",
    "  - this new and improved GPT model could perform `zero-shot learning (ZSL)`\n",
    "    - i.e. it could achieve good performance on many tasks `without any fine-tuning`\n",
    "- In 2021, Google's [Switch Transformers](https://homl.info/switch) used 1 trillion parameters\n",
    "- An unfortunate consequence of this trend toward gigantic models is that they cost lots of money and energy\n",
    "  - soon new ways are found to downsize transformers and make them more data-efficient\n",
    "  - such as the [DistilBERT model](https://homl.info/distilbert), a small and fast transformer model based on BERT, available on Hugging Face‚Äôs model hub\n",
    "    - it was trained using `distillation`:\n",
    "      - transferring knowledge from a teacher model to a student one\n",
    "      - which is usually much smaller than the teacher model\n",
    "- Many more transformer architectures came out after BERT almost on a monthly basis\n",
    "  - often improving on the state of the art across all NLP tasks\n",
    "- On November 30, 2022, [ChatGPT 3.5 (Chat Generative Pre-trained Transformer)](https://chat.openai.com/) was released\n",
    "- On December 6, 2023, Google released [Gemini](https://gemini.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers\n",
    "- first were used in generating image captions using [visual attention‚Å†](https://homl.info/visualattention)\n",
    "  - a CNN first processes the image and outputs some feature maps\n",
    "  - then a decoder RNN equipped with an attention mechanism generates the caption\n",
    "    - (p12) At each decoder time step (i.e., each word), the decoder uses the attention model to focus on just the right part of the image\n",
    "- then hybrids of CNN‚Äìtransformer architecture for object detection were proposed\n",
    "  - the CNN first processes the input images and outputs a set of feature maps\n",
    "  - then these feature maps are converted to sequences and fed to a transformer\n",
    "    - which outputs bounding box predictions\n",
    "- then a fully transformer-based vision model called a [vision transformer (ViT)](https://homl.info/vit) was introduced in October 2020\n",
    "  - it chops the image into little 16 √ó 16 squares, \n",
    "  - and treats the sequence of squares as if it were a sequence of word representations\n",
    "  - this model beat the state of the art on ImageNet image classification\n",
    "    - but had to use over 300 million additional images for training\n",
    "- Just two months later, [data-efficient image transformers (DeiTs)](https://homl.info/deit) achieved competitive results on ImageNet without requiring any additional data for training\n",
    "  - but used a distillation technique to transfer knowledge from state-of-the-art CNN models to this model\n",
    "- [the Perceiver architecture](https://homl.info/perceiver)\n",
    "  - a multimodal transformer, meaning you can feed it text, images, audio, or virtually any other modality\n",
    "- [DINO](https://homl.info/dino)\n",
    "  - an vision transformer capable of high-accuracy semantic segmentation\n",
    "  - trained entirely by self-supervision\n",
    "- [CLIP](https://homl.info/clip)\n",
    "  - proposed a large transformer model pretrained to match captions with images\n",
    "  - followed by [DALLE‚ãÖE](https://homl.info/dalle), then [DALLE‚ãÖE 2](https://homl.info/dalle2)\n",
    "- the [Flamingo paper](https://homl.info/flamingo) introduced a family of models pretrained on a wide variety of tasks across multiple modalities, including text, images, and videos\n",
    "  - A single model can be used across very different tasks, such as question answering, image captioning, and more\n",
    "- [GATO](https://homl.info/gato), a multimodal model that can be used as a policy for a reinforcement learning agent\n",
    "  - The same transformer can chat with you, caption images, play Atari games, control (simulated) robotic arms, and more, all with ‚Äúonly‚Äù 1.2 billion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face‚Äôs Transformers Library\n",
    "- There are many excellent pretrained models readily available for download via `TensorFlow Hub` or [Hugging Face‚Äôs model hub](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use a pretrained transformer\n",
    "# The simplest way to use the Transformers library is \n",
    "# to use the transformers.‚Äãpipe‚Å†line() function: \n",
    "#     just specify which task you want, such as sentiment analysis, \n",
    "#     and it downloads a default pretrained model ready to be used\n",
    "\n",
    "# 1) sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "# many other tasks are available on https://huggingface.co/tasks\n",
    "classifier = pipeline(\"sentiment-analysis\")  \n",
    "result = classifier(\"The actors were very convincing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Models can be very biased. # For example, it may like or dislike \n",
    "# some countries depending on the data it was trained on, and how it is used, \n",
    "# so use it with care:\n",
    "classifier([\"I am from India.\", \"I am from Iraq.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) specify a model instead of using the defaults\n",
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) load the same DistilBERT along with its corresponding tokenizer\n",
    "# the Transformers library provides many classes, including all sorts of \n",
    "# tokenizers, models, configurations, callbacks, and much more. \n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# then tokenize a couple of pairs of sentences\n",
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
    "                      padding=True, return_tensors=\"tf\")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)  pass this BatchEncoding object to the model; \n",
    "# it returns a TFSequenceClassifierOutput object containing its predicted class logits\n",
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) apply the softmax activation function to convert these logits to class probabilities\n",
    "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "Y_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) fine-tune the model\n",
    "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
