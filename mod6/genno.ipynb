{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod6/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod6/gen.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "Autoencoders, GANs, and Diffusion Models\n",
    "---\n",
    "_homl3 ch17_\n",
    "\n",
    "- `Autoencoders` are ANNs capable of learning `dense representations with a much lower dimensionality` of the input data, called `latent representations or codings`, without any supervision\n",
    "  - useful for visualization, feature detectors,  unsupervised pretraining of DNNs\n",
    "  - some autoencoders are generative models capable of `randomly generating new data` that looks very similar to the training data\n",
    "- `Generative adversarial networks (GANs)` are also ANNs capable of generating data\n",
    "  - widely used for super resolution, colorization, powerful image editing, \n",
    "  - turning simple sketches into photorealistic images, \n",
    "  - predicting the next frames in a video, \n",
    "  - augmenting a dataset, generating other types of data\n",
    "- `Diffusion Models` can generate more diverse and higher-quality images than GANs\n",
    "  - while also being much easier to train\n",
    "  - However, they are much slower to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab: Go to Runtime > Change runtime and select a GPU hardware\n",
    "# Kaggle: Go to Settings > Accelerator and select GPU\n",
    "# ‚ö†Ô∏è It may take more than one day to run the whole notebook without GPU\n",
    "import sys, os, math, copy\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install -q -U transformers\n",
    "    %pip install -q -U datasets\n",
    "else:\n",
    "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds\n",
    "import tensorflow as tf, tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Demo\n",
    "---\n",
    "- [This person does not exist](https://thispersondoesnotexist.com/)\n",
    "  - refresh the webpage and look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autoencoders, GANs, and diffusion models are `unsupervised` and all learn `latent representations`\n",
    "  - have many similar applications such as `generative` models\n",
    "- But they work very differently\n",
    "  - Autoencoders learn efficient ways of representing the data by constraints on learning the identity function\n",
    "  - GANs are composed of two neural networks: a `generator` that tries to generate data that looks similar to the training data, and a `discriminator` that tries to tell real data from fake data\n",
    "  - A `denoising diffusion probabilistic model (DDPM)` is trained to remove a tiny bit of noise from an image\n",
    "    - repeatedly run the diffusion model on a noisy image\n",
    "      - a high-quality image will gradually emerge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "- (p1) always composed of two parts:\n",
    "  - an `encoder (or recognition network)` that converts the inputs to a latent representation\n",
    "  - a `decoder (or generative network)` that converts the internal representation to the outputs\n",
    "- typically has the same architecture as a multilayer perceptron (MLP)\n",
    "  - except that the number of neurons in the output layer must be equal to the number of inputs\n",
    "- The outputs are often called the `reconstructions` \n",
    "  - because the autoencoder tries to reconstruct the inputs\n",
    "- The cost function contains a `reconstruction loss` that penalizes the model when the reconstructions are different from the inputs\n",
    "- The autoencoder is said to be `undercomplete` if \n",
    "  - its internal representation has a lower dimensionality than the input data\n",
    "  - so it is forced to learn the `most important features` in the input data\n",
    "    - and drop the unimportant ones\n",
    "- If the coding layer is equal to or larger than the inputs\n",
    "  - the autoencoder is called `overcomplete`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üçé Example 1\n",
    "---\n",
    "Given two number sequences below, which one is easier to remember?\n",
    "- 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\n",
    "  - need to remember 10 random 2-digit numbers\n",
    "- 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\n",
    "  - `even numbers from 50 to 14`\n",
    "  - only 50 and 14 two 2-digit numbers need to be remembered after recognizing its pattern\n",
    "  - a condensed and effective `latent representation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Performing PCA with an Undercomplete Linear Autoencoder\n",
    "# 1) An autoencoder ends up performing principal component analysis (PCA) if \n",
    "#   it uses only linear activations \n",
    "#   and the cost function is the mean squared error (MSE)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "encoder = tf.keras.Sequential([tf.keras.layers.Dense(2)])\n",
    "decoder = tf.keras.Sequential([tf.keras.layers.Dense(3)])\n",
    "autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.5)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) (p2) builds the same 3D dataset as in Chapter 8 \n",
    "\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "m = 60\n",
    "X = np.zeros((m, 3))  # initialize 3D dataset\n",
    "\n",
    "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
    "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
    "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
    "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
    "X_train = X + [0.2, 0, 0.2]  # shift a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) train the model then encoding\n",
    "# X_train is used as both the inputs and the targets\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, epochs=500, verbose=False)\n",
    "codings = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (p2) visualize the latent space\n",
    "fig1, ax1 = plt.subplots(figsize=(4,3))\n",
    "ax1.plot(codings[:,0], codings[:, 1], \"b.\")\n",
    "ax1.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax1.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "ax1.grid(True)\n",
    "ax1.set_title(\"2D codings of the 3D training set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (p2) shows the original 3D dataset and the output of the autoencoder‚Äôs hidden layer\n",
    "  - i.e., the coding layer\n",
    "- the autoencoder found the `best 2D plane` to project the data onto\n",
    "   - `preserving as much variance` in the data as it could (just like PCA)\n",
    " - it is considered as performing a form of `self-supervised` learning\n",
    "   - since it is based on a supervised learning technique with automatically generated labels\n",
    "     - in this case simply equal to the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders\n",
    "- have `multiple hidden` layers\n",
    "  - also called `deep autoencoders`\n",
    "  - more layers help the autoencoder learn more complex codings\n",
    "    - ‚ö†Ô∏è be aware of overfitting without learning any useful patterns\n",
    "- (p3) typically symmetrical with regard to the central hidden layer (the coding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build a Stacked Autoencoder Using Keras\n",
    "#  \n",
    "# 1) loads, scales, and splits the fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) build a stacked Autoencoder \n",
    "# with 3 hidden layers and 1 output layer # (i.e., 2 stacked Autoencoders).\n",
    "# much like a regular deep MLP\n",
    "# a) The encoder takes inputs 28 √ó 28‚Äìpixel grayscale images\n",
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "# b) The decoder takes inputs codings of size 30 (output by the encoder)\n",
    "# reshapes the final vectors into 28 √ó 28 arrays \n",
    "# so the decoder‚Äôs outputs have the same shape as the encoder‚Äôs inputs\n",
    "\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) train the stacked Autoencoder                   \n",
    "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Visualizing the Reconstructions\n",
    "# The reconstructions are recognizable, but a bit too lossy\n",
    "# but remember that the images were compressed down \n",
    "#   to just 30 numbers, instead of 784\n",
    "def plot_reconstructions(model, images=X_valid, n_images=5,\n",
    "                         title=\"Original images (top) and their reconstructions (bottom)\"):\n",
    "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    plt.title(title)\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_reconstructions(stacked_ae);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visualizing the Fashion MNIST Dataset\n",
    "# compared to other dimensionality reduction algorithms\n",
    "#   autoencoders can handle large datasets \n",
    "#    with many instances and many features\n",
    "# A strategy of visualization\n",
    "# a) use an autoencoder to reduce the dimensionality down to a reasonable level\n",
    "# b) then use another dimensionality reduction algorithm for visualization\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# a) use the stacked autoencoder to reduce the dimensionality down to 30\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "# b) then use t-SNE to reduce the dimensionality down to 2 for visualization\n",
    "tsne = TSNE(init=\"pca\", learning_rate=\"auto\")\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) visualize\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) better visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "Z = X_valid_2D\n",
    "Z = (Z - Z.min()) / (Z.max() - Z.min())  # normalize to the 0-1 range\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=y_valid, s=10, cmap=cmap)\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(Z):\n",
    "    dist = ((position - image_positions) ** 2).sum(axis=1)\n",
    "    if dist.min() > 0.02: # if far enough from other images\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n",
    "        plt.gca().add_artist(imagebox)\n",
    "\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Pretraining Using Stacked Autoencoders\n",
    "---\n",
    "- (p4) first train a stacked autoencoder using all the data\n",
    "  - this is a large dataset but most of it is `unlabeled`\n",
    "- then `reuse the lower layers` to create a neural network for the actual task \n",
    "  - and train it using the labeled data\n",
    "  - freeze the pretrained layers (at least the lower ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two techniques for training stacked autoencoders\n",
    "---\n",
    "- Tying weights\n",
    "  - Tying the weights of the decoder layers to the weights of the encoder layers\n",
    "    - halves the number of weights in the model\n",
    "    - speeds up training and limits the risk of overfitting\n",
    "- (p5) Training one autoencoder at a time\n",
    "  - train one shallow autoencoder at a time\n",
    "    - encode the whole training set using this autoencoder to get a new (compressed) training set\n",
    "    - then train the next autoencoder on this new dataset and so on\n",
    "  - finally stack all these autoencoders into a single stacked autoencoder\n",
    "    - first stack the hidden layers of each autoencoder\n",
    "    - then the output layers in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tying weights\n",
    "# \n",
    "class DenseTranspose(tf.keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = dense\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\",\n",
    "                                      shape=self.dense.input_shape[-1],\n",
    "                                      initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(Z + self.biases)\n",
    "\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "dense_2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "\n",
    "tied_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = tf.keras.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"relu\"),\n",
    "    DenseTranspose(dense_1),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = tf.keras.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "# compiles the model\n",
    "tied_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train the model\n",
    "history = tied_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) show the reconstructions\n",
    "plot_reconstructions(tied_ae)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Training one autoencoder at a time\n",
    "# 1) train autoencoder\n",
    "def train_autoencoder(n_neurons, X_train, X_valid, n_epochs=10,\n",
    "                      output_activation=None):\n",
    "    n_inputs = X_train.shape[-1]\n",
    "    encoder = tf.keras.layers.Dense(n_neurons, activation=\"relu\")\n",
    "    decoder = tf.keras.layers.Dense(n_inputs, activation=output_activation)\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "    autoencoder.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "    autoencoder.fit(X_train, X_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid, X_valid))\n",
    "    return encoder, decoder, encoder(X_train), encoder(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train 2 autoencoders\n",
    "X_train_flat = tf.keras.layers.Flatten()(X_train)\n",
    "X_valid_flat = tf.keras.layers.Flatten()(X_valid)\n",
    "enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(\n",
    "    100, X_train_flat, X_valid_flat)\n",
    "enc2, dec2, _, _ = train_autoencoder(\n",
    "    30, X_train_enc1, X_valid_enc1, output_activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) stack the two autoencoders\n",
    "stacked_ae_1_by_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    enc1, enc2, dec2, dec1,\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) visualize the stacked autoencoder's generated fashions\n",
    "plot_reconstructions(stacked_ae_1_by_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoders\n",
    "- the previous autoencoders are built with normal DNNs\n",
    "  - work with very small images\n",
    "- it is natural to build [autoencoders with CNNs](https://homl.info/convae) (convolutional autoencoders) for large images\n",
    "  - The encoder is a regular CNN composed of convolutional layers and pooling layers\n",
    "    - reduces the spatial dimensionality of the inputs (i.e., height and width)\n",
    "    - increases the depth (i.e., the number of feature maps)\n",
    "  - The decoder must do the reverse\n",
    "    - upscales the image and reduce its depth back to the original dimensions\n",
    "      - with transpose convolutional layers \n",
    "      - by combining upsampling layers with convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a convolutional autoencoder for Fashion MNIST\n",
    "# 1) with 3 hidden layers and 1 output layer \n",
    "# \n",
    "\n",
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 √ó 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 √ó 7 x 32\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 √ó 3 x 64\n",
    "    \n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "# compiles the model\n",
    "conv_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train the model\n",
    "history = conv_ae.fit(X_train, X_train, epochs=10,\n",
    "                      validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) shows the reconstructions\n",
    "plot_reconstructions(conv_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent autoencoders\n",
    "---\n",
    "- built with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a recurrent autoencoder\n",
    "# treat each Fashion MNIST image as a sequence of 28 vectors, \n",
    "#   each with 28 dimensions:\n",
    "\n",
    "recurrent_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(30)\n",
    "])\n",
    "recurrent_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.RepeatVector(28),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.Dense(28)\n",
    "])\n",
    "recurrent_ae = tf.keras.Sequential([recurrent_encoder, recurrent_decoder])\n",
    "recurrent_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train the model\n",
    "history = recurrent_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) show the reconstructions\n",
    "plot_reconstructions(recurrent_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More types of autoencoders\n",
    "---\n",
    "- Denoising autoencoders\n",
    "- Sparse autoencoders\n",
    "- Variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Denoising Autoencoders](https://homl.info/114)\n",
    "---\n",
    "- (p6) The noise on the inputs can be \n",
    "  - pure `Gaussian noise`\n",
    "  - or `randomly switched-off` inputs just like in dropout\n",
    "- (p7) implemented by applying to the encoder‚Äôs inputs\n",
    "  - a GaussianNoise layer\n",
    "  - or an additional Dropout layer\n",
    "  - both layers are only active during training to add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a denoising autoencoder with dropout\n",
    "dropout_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "])\n",
    "dropout_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])\n",
    "\n",
    "# 2) compiles the model\n",
    "dropout_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) train the model\n",
    "history = dropout_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) denoisy inputs\n",
    "dropout = tf.keras.layers.Dropout(0.5)\n",
    "plot_reconstructions(dropout_ae, dropout(X_valid, training=True),\n",
    "                     title=\"Noisy images (top) and their cleaned (bottom)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) denoisy inputs obscured by Gaussian noise\n",
    "noise = tf.keras.layers.GaussianNoise(0.2)\n",
    "plot_reconstructions(dropout_ae, noise(X_valid, training=True),\n",
    "                     title=\"Noisy images (top) and their cleaned (bottom)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse autoencoders (SAEs)\n",
    "---\n",
    "- often have good feature extraction by applying the constraint of `sparsity`\n",
    "- reduce the number of active neurons in the coding layer by adding an appropriate term to the cost function\n",
    "  - each neuron in the coding layer typically ends up representing a useful feature\n",
    "- A simple approach of introducing sparsity\n",
    "  - the encoders\n",
    "    - use the `sigmoid` activation function in the coding layer\n",
    "      - to limit the codings in range (0,1)\n",
    "    - use a large coding layer with some ‚Ñì‚ÇÅ regularization\n",
    "      - to preserve the most important codings while eliminating unimportant ones\n",
    "  - the decoders are normal decoder\n",
    "- Another approach of introducing sparsity\n",
    "  - measure the actual sparsity of the coding layer at each training iteration\n",
    "    - by computing the average activation of each neuron in the coding layer over the whole training batch\n",
    "  - penalize the model when the measured sparsity differs from a target sparsity\n",
    "    - (p8) by adding to the cost function a `sparsity loss` such as\n",
    "      - the mean squared error (MSE)\n",
    "      - the mean absolute error (MAE)\n",
    "      - or the Kullback‚ÄìLeibler (KL) divergence\n",
    "        - which has much stronger gradients than MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence\n",
    "---\n",
    "- ${D_{KL}(P \\Vert Q) }$ between two discrete probability distributions P and Q can be computed\n",
    "  - ${\\displaystyle D_{KL}(P \\Vert Q) = \\sum_{i}P(i)\\log\\dfrac{P(i)}{Q(i)} }$\n",
    "- Measuring the divergence between the `target probability p` that a neuron in the coding layer will activate and the `actual probability q` estimated by measuring the `mean activation over the training batch` by:\n",
    "  - ${\\displaystyle D_{KL}(p \\Vert q) = p\\log\\dfrac{p}{q} + (1-p)\\log\\dfrac{(1-p)}{(1-q)} }$\n",
    "  - then sum up all neuron's losses and add the result to the cost function\n",
    "- Multiplying the sparsity loss by a sparsity weight hyperparameter can control the `relative importance` of the sparsity loss and the reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a SAE with ‚Ñì‚ÇÅ regularization\n",
    "# 1) use the sigmoid activation function in the coding layer. \n",
    "# also add ‚Ñì‚ÇÅ regularization to it by adding an `ActivityRegularization` layer \n",
    "# after the coding layer. Alternatively, \n",
    "# we could add `activity_regularizer=tf.keras.regularizers.l1(1e-4)` \n",
    "# to the coding layer itself.\n",
    "\n",
    "sparse_l1_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.ActivityRegularization(l1=1e-4)\n",
    "])\n",
    "sparse_l1_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "sparse_l1_ae = tf.keras.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
    "\n",
    "#  compiles the model\n",
    "sparse_l1_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) train the model\n",
    "history = sparse_l1_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) shows the reconstructions\n",
    "plot_reconstructions(sparse_l1_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. build a SAE with KL-Divergence regularization\n",
    "# 1) define a custom regularizer for KL-Divergence regularization:\n",
    "\n",
    "kl_divergence = tf.keras.losses.kullback_leibler_divergence\n",
    "\n",
    "class KLDivergenceRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, weight, target):\n",
    "        self.weight = weight\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        mean_activities = tf.reduce_mean(inputs, axis=0)\n",
    "        return self.weight * (\n",
    "            kl_divergence(self.target, mean_activities) +\n",
    "            kl_divergence(1. - self.target, 1. - mean_activities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) use this regularizer to push the model to have \n",
    "# about 10% sparsity in the coding layer:\n",
    "kld_reg = KLDivergenceRegularizer(weight=5e-3, target=0.1)\n",
    "sparse_kl_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(300, activation=\"sigmoid\",\n",
    "                          activity_regularizer=kld_reg)\n",
    "])\n",
    "sparse_kl_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "sparse_kl_ae = tf.keras.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n",
    "\n",
    "# compiles the model\n",
    "sparse_kl_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) train the model\n",
    "history = sparse_kl_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) shows the reconstructions\n",
    "plot_reconstructions(sparse_kl_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Variational autoencoders (VAEs)](https://homl.info/115)\n",
    "---\n",
    "- `probabilistic` autoencoders whose outputs are partly determined by chance\n",
    "- `generative` autoencoders that can generate new instances that look like training instances\n",
    "- (p9) perform `variational Bayesian inference`\n",
    "  - the encoder learns a mean coding Œº and a standard deviation œÉ\n",
    "  - The actual coding is then sampled randomly from a Gaussian distribution ${\\displaystyle {\\mathcal {N}}(\\mu ,\\sigma ^{2})}$\n",
    "  - the decoder decodes the `sampled coding` normally\n",
    "  - the final output `resembles` the training instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE cost function\n",
    "---\n",
    "composed of two parts:\n",
    "- ‚ù∂ the usual `reconstruction loss` that pushes the autoencoder to reproduce its inputs\n",
    "  - can be measured with `MSE`\n",
    "- ‚ù∑ the `latent loss` that pushes the autoencoder to have codings that look sampled from a simple Gaussian distribution\n",
    "  - can be measured with the `KL divergence` between the target distribution and the actual distribution of the codings\n",
    "  - computed by ${\\displaystyle ‚Ñí = -\\dfrac{1}{2}\\sum_{i=1}^{n}\\left( 1+\\log(œÉ_i^2) - œÉ_i^2 - Œº_i^2 \\right) }$\n",
    "    - `n` is the codings‚Äô `dimensionality`\n",
    "    - `Œº·µ¢ and œÉ·µ¢` are the `mean and standard deviation` of the i·µó ∞ component of the codings\n",
    "    - The vectors $\\boldsymbol{Œº}$ and $\\boldsymbol{œÉ}$ (which contain all the Œº·µ¢ and œÉ·µ¢) are output by the encoder\n",
    "  - A common tweak to the VAE‚Äôs architecture is to make the encoder output ${Œ≥ = \\log(\\boldsymbol{œÉ}^2)}$ rather than $\\boldsymbol{œÉ}$\n",
    "    - ${\\displaystyle ‚Ñí = -\\dfrac{1}{2}\\sum_{i=1}^{n}\\left( 1+ Œ≥_i - e^{Œ≥_i} - Œº_i^2 \\right) }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a VAE for Fashion MNIST\n",
    "# 1) define a custom layer to sample the codings, given Œº and Œ≥\n",
    "# This samples a codings vector from the Gaussian distribution N(Œº,œÉ¬≤)\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) create the encoder using the functional API\n",
    "codings_size = 10\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[28, 28])\n",
    "Z = tf.keras.layers.Flatten()(inputs)\n",
    "Z = tf.keras.layers.Dense(150, activation=\"relu\")(Z)\n",
    "Z = tf.keras.layers.Dense(100, activation=\"relu\")(Z)\n",
    "codings_mean = tf.keras.layers.Dense(codings_size)(Z)  # Œº\n",
    "codings_log_var = tf.keras.layers.Dense(codings_size)(Z)  # Œ≥\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = tf.keras.Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "# 3) create the decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[codings_size])\n",
    "x = tf.keras.layers.Dense(100, activation=\"relu\")(decoder_inputs)\n",
    "x = tf.keras.layers.Dense(150, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dense(28 * 28)(x)\n",
    "outputs = tf.keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = tf.keras.Model(inputs=[decoder_inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) build the VAE\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = tf.keras.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "# add the latent loss\n",
    "latent_loss = -0.5 * tf.reduce_sum(\n",
    "    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\n",
    "    axis=-1)\n",
    "variational_ae.add_loss(tf.reduce_mean(latent_loss) / 784.)\n",
    "\n",
    "# compile the model\n",
    "variational_ae.compile(loss=\"mse\", optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) train the model\n",
    "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
    "                             validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) show reconstructions\n",
    "plot_reconstructions(variational_ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generating Fashion MNIST Images with VAE\n",
    "# 1) use the VAE above to generate images that look like fashion items\n",
    "# by sampling random codings from a Gaussian distribution then decoding them\n",
    "\n",
    "codings = tf.random.normal(shape=[3 * 7, codings_size])\n",
    "images = variational_decoder(codings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) show the generated fashions\n",
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = images.squeeze(axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_multiple_images(images, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic interpolation\n",
    "---\n",
    "- can be done with VAEs at the codings level instead of at the pixel level\n",
    "- ex. take a few codings along an `arbitrary line in latent space` and decode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Semantic interpolation\n",
    "# get a sequence of images that gradually go from one fashion to another\n",
    "codings = np.zeros([7, codings_size])\n",
    "codings[:, 6] = np.linspace(0.8, -0.8, 7)  # axis 3 looks best in this case\n",
    "images = variational_decoder(codings).numpy()\n",
    "\n",
    "plot_multiple_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Generative Adversarial Networks](https://homl.info/gan)\n",
    "---\n",
    "- Idea:  \n",
    "  - make neural networks compete against each other \n",
    "  - in the hope that this competition will push them to excel\n",
    "- Structure: (p10) a GAN is composed of two neural networks:\n",
    "  - Generator\n",
    "    - offers the same functionality as a decoder in a VAE\n",
    "    - takes as input a random distribution and outputs some data typically an image\n",
    "      - the random input is the `latent representation` of the image to be generated\n",
    "  - Discriminator\n",
    "    - takes as input either a fake image from the generator \n",
    "      - or a real image from the training set\n",
    "    - then guesses whether the input image is fake or real\n",
    "- Training:\n",
    "  - the generator and the discriminator have opposite goals\n",
    "    - the discriminator tries to tell fake images from real images\n",
    "    - the generator tries to produce images that look real enough to trick the discriminator\n",
    "  - ‚à¥ cannot be trained like a regular neural network\n",
    "- Each training iteration is divided into two phases:\n",
    "  - ‚ù∂ train the `discriminator` as a `binary classifier`\n",
    "    - the inputs are real images from the training set and fake images produced by the generator\n",
    "    - the labels are set to 0 for fake images and 1 for real images\n",
    "    - backpropagation only optimizes the `weights of the discriminator`\n",
    "      - the generator's weights are frozen during this phase\n",
    "  - ‚ù∑ train the generator\n",
    "    - first the generator is used to produce `another` batch of fake images\n",
    "    - then the discriminator is used to tell whether these `new` images are fake or real\n",
    "      - all the new images are labeled as 1 so that the discriminator will wrongly believe to be real\n",
    "    - similarly, backpropagation only updates the weights of the generator\n",
    "      - the weights of the discriminator are frozen during this phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a simple GAN for Fashion MNIST\n",
    "# 1) build the generator and the discriminator. \n",
    "\n",
    "codings_size = 30\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "# The generator is similar to an autoencoder‚Äôs decoder, \n",
    "generator = tf.keras.Sequential([\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "# the discriminator is a regular binary classifier\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) The gan model is also a binary classifier\n",
    "# the generator will only be trained through the gan model, \n",
    "# so we do not need to compile it at all.\n",
    "\n",
    "# the discriminator is trainable by itself\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# the discriminator should not be trained during the second phase\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) create the training scheme\n",
    "\n",
    "# a) create a Dataset to iterate through the images\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "# repeat the two training phases\n",
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\") \n",
    "        for X_batch in dataset:\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            \n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            \n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            gan.train_on_batch(noise, y2)\n",
    "        # plot images during training\n",
    "        plot_multiple_images(generated_images.numpy(), 8)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) train the GAN\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) generate new images by feeding the generator \n",
    "# with randomly sampled codings from a Gaussian distribution\n",
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(codings)\n",
    "\n",
    "# show the image\n",
    "plot_multiple_images(generated_images, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Difficulties of Training GANs\n",
    "---\n",
    "- the training is a `zero-sum` game that the generator and the discriminator constantly try to outsmart each other\n",
    "- the game may end up in a state `Nash equilibrium` named by game theorists\n",
    "  - when no player would be better off changing their own strategy\n",
    "    - assuming the other players do not change theirs\n",
    "- Different initial states and dynamics may lead to one equilibrium or the other\n",
    "  - a GAN can only reach a single Nash equilibrium\n",
    "    - when the generator produces perfectly realistic images \n",
    "    - and the discriminator is forced to guess randomly\n",
    "  - However, nothing guarantees the equilibrium will ever be reached\n",
    "- The biggest difficulty is called `mode collapse`\n",
    "  - when the generator‚Äôs outputs gradually become less diverse\n",
    "  - `ex`. Suppose that the generator gets better at producing convincing shoes than any other class\n",
    "    - It will fool the discriminator a bit more with shoes, and this will encourage it to produce even more images of shoes\n",
    "    - Gradually, it will `forget` how to produce anything else\n",
    "  - Meanwhile, the only fake images that the discriminator will see will be shoes\n",
    "    - so it will also forget how to discriminate fake images of other classes\n",
    "  - Eventually, when the discriminator manages to discriminate the fake shoes from the real ones\n",
    "    - the generator will be forced to move to another class\n",
    "    - It may then become good at shirts, forgetting about shoes, and the discriminator will follow\n",
    "  - The GAN may gradually `cycle across a few classes`, never really becoming very good at any of them\n",
    "- Moreover, GAN's parameters may end up `oscillating and becoming unstable`\n",
    "  - because the generator and the discriminator are constantly pushing against each other\n",
    "- GANs are `very sensitive` to the hyperparameters\n",
    "  - you may have to spend a lot of effort fine-tuning them\n",
    "  - ex. optimizer `RMSProp` is better than `Nadam` in the GAN above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches for training GANs\n",
    "---\n",
    "- [experience replay](https://homl.info/gansequal) stores images produced by the generator at each iteration in a cycle buffer\n",
    "  - training the discriminator using real images plus fake images drawn from this buffer rather than just fake images produced by the current generator\n",
    "  - This reduces the chances that the discriminator will overfit the latest generator‚Äôs outputs\n",
    "- `mini-batch discrimination` measures how similar images are across the batch and provides this statistic to the discriminator \n",
    "  - so it can easily reject a whole batch of fake images that lack diversity\n",
    "  - This encourages the generator to produce a greater `variety` of images\n",
    "    - reducing the chance of mode collapse\n",
    "- this is still a very active field of research \n",
    "   - the dynamics of GANs are still not perfectly understood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two more powerful and more complex GANs\n",
    "---\n",
    "- Deep Convolutional GANs\n",
    "- StyleGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional GANs\n",
    "---\n",
    "- abbreviated as [DCGANs](https://homl.info/dcgan)\n",
    "- The main guidelines for building stable convolutional GANs:\n",
    "  - Replace any pooling layers with `strided` convolutions in the `discriminator`\n",
    "    - and `transposed` convolutions in the `generator`\n",
    "  - Use `batch normalization` in both the generator and the discriminator\n",
    "    - except in the generator‚Äôs output layer and the discriminator‚Äôs input layer\n",
    "  - Remove fully connected hidden layers for deeper architectures\n",
    "  - Use `ReLU` activation in the generator for all layers \n",
    "    - except the output layer which should use `tanh`\n",
    "  - Use leaky `ReLU` activation in the discriminator for all layers\n",
    "- These guidelines will work in many cases but not always\n",
    "  - better experiment with different hyperparameters\n",
    "  - even just changing the random seed and training the exact same model again will sometimes work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. build a small DCGAN with Fashion MNIST\n",
    "# Feel free to tweak this architecture: \n",
    "#   you will see how sensitive it is to the hyperparameters\n",
    "#   especially the relative learning rates of the two networks\n",
    "\n",
    "codings_size = 100\n",
    "\n",
    "# image size: 7x7 ‚Üí 14x14 ‚Üí 28x28\n",
    "# depth: 128 ‚Üí 64 ‚Üí 1\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "# The discriminator looks much like a regular CNN \n",
    "#   for binary classification, except instead of \n",
    "#   using max pooling layers to downsample the image\n",
    "\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) compiles the discrimator and the gan, as earlier\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) reshape to add the channel dimension\n",
    "# and rescale from [-1,1] to [0,1]\n",
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) generates the dataset and trains the GAN, just like earlier\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) generate some fashions with the DCGAN above\n",
    "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(noise)\n",
    "plot_multiple_images(generated_images, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÉ Exercise\n",
    "---\n",
    "- Scale up the GAN above and train it on a large dataset of faces, flowers, or animals\n",
    "  - Can you get fairly realistic images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN semantic operation\n",
    "---\n",
    "- (p11) DCGANs can learn quite `meaningful latent representations`\n",
    "  - each image is generated from a `coding vector`\n",
    "  - each column: the bottom image is generated from the average of the codings of the images above it\n",
    "  - bottom row: arithmetic operations on the codings imply semantic operations on the images they represent\n",
    "    - The eight other images around it were generated based on the same vector plus a bit of `noise`\n",
    "- [Conditional GANs (CGANs)](https://homl.info/cgan) can generate classes of images with image class as an extra input\n",
    "  - trained by adding each image's class as an extra input to both the generator and the discriminator\n",
    "    - they will both learn what each class looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Progressive Growing of GANs](https://homl.info/progan)\n",
    "---\n",
    "- gradually `adds convolutional layers` to both the generator and the discriminator to produce larger and larger images\n",
    "  - The extra layers get added at the end of the generator \n",
    "    - and at the beginning of the discriminator\n",
    "  - previously trained layers remain trainable\n",
    "- ex. (p12) grows the generator‚Äôs outputs from `4 √ó 4` to `8 √ó 8`\n",
    "  - The final outputs are a weighted sum = `the new outputs√óŒ±` + `the original outputs√ó(1 ‚Äì Œ±)` \n",
    "  - slowly increasing Œ± from 0 to 1 to fade in the new outputs and fade out the original\n",
    "- Other techniques improving GANs\n",
    "  - `Mini-batch standard deviation layer`\n",
    "    - computes the standard deviation across all channels and all instances in each batch\n",
    "    - added near the end of the discriminator to help it discern generator's monotony\n",
    "    - This will encourage the generator to produce more diverse outputs\n",
    "      - reducing the risk of mode collapse\n",
    "  - `Equalized learning rate`\n",
    "    - initializes all weights using normal distribution ‚Ñï(0,1) rather than using He initialization\n",
    "    - meanwhile, the weights are scaled down at runtime by dividing by ${\\displaystyle \\sqrt{\\dfrac{2}{n_{layerInputs}}}}$\n",
    "    - This ensures that the `dynamic range` is the same for all parameters throughout training\n",
    "      - This both speeds up and stabilizes training\n",
    "  - `Pixelwise normalization layer`\n",
    "    - added after each convolutional layer in the generator\n",
    "    - normalizes each activation based on all the activations in the same image and at the same location but across all channels\n",
    "      - by dividing by the square root of the mean squared activation\n",
    "    - This avoids explosions in the activations due to excessive competition between the generator and the discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StyleGANs](https://homl.info/stylegan)\n",
    "---\n",
    "- able to generate state-of-the-art high-resolution images\n",
    "  - with the same local structure as the training images at every scale\n",
    "- (p13) A StyleGAN generator is composed of two networks\n",
    "  - `Mapping network` maps the codings to multiple style vectors\n",
    "    - An eight-layer MLP that maps the latent representations ${\\mathbf{z}}$ to a vector ${\\mathbf{w}}$\n",
    "    - this ${\\mathbf{w}}$ is then sent through multiple `affine transformations` (i.e Dense layers with no activation functions, represented by the \"A\" boxes) to produce multiple style vectors\n",
    "    - These style vectors control the style of the generated image at different levels\n",
    "      - from fine-grained texture  (e.g., hair color) \n",
    "      - to high-level features (e.g., adult or child)\n",
    "  - `Synthesis network` are responsible for generating the images\n",
    "    - It processes a constant learned input through multiple convolutional and upsampling layers with two twists\n",
    "      - First, some noise is added to the input and to all the outputs of the convolutional layers  (before the activation function)\n",
    "      - Second, each noise layer is followed by an `adaptive instance normalization (AdaIN)` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°Demo\n",
    "---\n",
    "- [Alias-Free Generative Adversarial Networks (StyleGAN3)](https://nvlabs.github.io/stylegan3/)\n",
    "- [CycleGAN](https://github.com/junyanz/CycleGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Models\n",
    "---\n",
    "- [Original idea appeared in 2015]((https://homl.info/diffusion))\n",
    "  - train a model to learn the reverse process of diffusion\n",
    "  - unfortunately shadowed by GAN back then\n",
    "- In 2020, [denoising diffusion probabilistic model (DDPM)](https://homl.info/ddpm) can generate highly realistic images\n",
    "  - improved [in 2021](https://homl.info/ddpm2) and finally beat GANs\n",
    "- Compared to GANs and VAEs\n",
    "  - much easier to train\n",
    "  - the generated images are more diverse and of even higher quality\n",
    "  - but slower to generate images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How a DDPM works?\n",
    "---\n",
    "- (p14) `Forward process` linearly drowns the input image with `Gaussian noise ‚Ñï(0,Œ≤‚Çú)`\n",
    "  - this noise is independent for each pixel, which is called `isotropic`\n",
    "  - the input image fades linearly between time steps 0 and T (=2000‚àº4000)\n",
    "  - the `mean` of the pixel values gradually approaches 0\n",
    "    - since the pixel values get rescaled slightly at each step by a factor of ${\\sqrt{1-Œ≤_t}}$\n",
    "    - the image signal ${ \\bar{Œ±}_t = Œ±_1 Œ±_2 Œ±_3‚ãØ Œ±_t = ‚àè_{i=1}^t Œ±_i }$ ‚Üí 0\n",
    "  - the `variance` will gradually converge to 1 since Œ≤‚Çú is a bit smaller than 1\n",
    "    - since the standard deviation of the pixel values also gets scaled by ${\\sqrt{Œ±_t}}$\n",
    "      - where Œ±‚Çú is the remaining signal variance `Œ±‚Çú = 1 - Œ≤‚Çú`\n",
    "    - (p15) The `variance schedule` for the forward diffusion process is specified as\n",
    "      - ${ Œ≤_t = 1 - \\dfrac{Œ±_t}{Œ±_{t-1}} }$ with ${ Œ±_t = \\dfrac{f(t)}{f(0)} }$ and ${ f(t)=cos\\left(\\dfrac{t/T+s}{1+s}‚ãÖ\\dfrac{œÄ}{2} \\right)^2 }$\n",
    "      - s is a tiny value 0.008 by default which prevents Œ≤‚Çú from being too small near t = 0\n",
    "      - Œ≤‚Çú is clipped to be no larger than 0.999 to avoid instabilities near t = T\n",
    "  - the `probability distribution q` of the forward diffusion process is given by\n",
    "    - ${\\displaystyle q(\\mathbf{x}_t | \\mathbf{x}_{t-1}) = \\mathcal{N} (\\sqrt{1-Œ≤_t} \\mathbf{x}_{t-1}, Œ≤_t\\mathbf{I} ) }$\n",
    "  - all the noise can be added in just one shot to ${\\mathbf{x}_t}$ since the sum of multiple Gaussian distributions is also a Gaussian distribution by\n",
    "    - ${ \\displaystyle q(\\mathbf{x}_t | \\mathbf{x}_0) = \\mathcal{N} \\left(\\sqrt{Œ±_t} \\mathbf{x}_0, (1-Œ±_t)\\mathbf{I} \\right)  }$\n",
    "    - this is a shortcut, so no need to calculate ${\\mathbf{x}_1, \\mathbf{x}_2, ‚ãØ, \\mathbf{x}_{t-1}}$ first\n",
    "- `Reverse process` goes from ${\\mathbf{x}_t}$ to ${\\mathbf{x}_{t-1}}$ removing a tiny bit of noise from an image by\n",
    "  - ${\\displaystyle \\mathbf{x}_{t-1} = \\dfrac{1}{\\sqrt{Œ±_t}}\\left(\\mathbf{x}_t - \\dfrac{Œ≤_t}{\\sqrt{1-Œ±_t}}\\boldsymbol{Œµ_Œ∏}\\left(\\mathbf{x}_t,t \\right) \\right) + \\sqrt{Œ≤_t}\\mathbf{z} }$\n",
    "    - ${\\boldsymbol{Œµ_Œ∏}\\left(\\mathbf{x}_t,t \\right)}$ represents the noise predicted by the model given the input image ${\\mathbf{x}_t}$ and the time step t\n",
    "    - The $\\boldsymbol{Œ∏}$ represents the model parameters\n",
    "    - ${\\mathbf{z}}$ is Gaussian noise with mean 0 and variance 1 which makes the reverse process stochastic: \n",
    "      - if you run it multiple times, you will get different images\n",
    "  - there is no shortcut to get ${\\mathbf{x}_0}$\n",
    "    - so repeats the operation many times until all the noise is gone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a variance schedule\n",
    "def variance_schedule(T, s=0.008, max_beta=0.999):\n",
    "    t = np.arange(T + 1)\n",
    "    f = np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2\n",
    "    alpha = np.clip(f[1:] / f[:-1], 1 - max_beta, 1)\n",
    "    alpha = np.append(1, alpha).astype(np.float32)  # add Œ±‚ÇÄ = 1\n",
    "    beta = 1 - alpha\n",
    "    alpha_cumprod = np.cumprod(alpha)\n",
    "    return alpha, alpha_cumprod, beta  # Œ±‚Çú , Œ±ÃÖ‚Çú , Œ≤‚Çú for t = 0 to T\n",
    "\n",
    "T = 4000\n",
    "alpha, alpha_cumprod, beta = variance_schedule(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. add noise to image\n",
    "#  take a batch of clean images from the dataset and prepare them\n",
    "def prepare_batch(X):\n",
    "    X = tf.cast(X[..., tf.newaxis], tf.float32) * 2 - 1  # scale from ‚Äì1 to +1\n",
    "    X_shape = tf.shape(X)\n",
    "    t = tf.random.uniform([X_shape[0]], minval=1, maxval=T + 1, dtype=tf.int32)\n",
    "    alpha_cm = tf.gather(alpha_cumprod, t)\n",
    "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
    "    noise = tf.random.normal(X_shape)\n",
    "    return {\n",
    "        \"X_noisy\": alpha_cm ** 0.5 * X + (1 - alpha_cm) ** 0.5 * noise,\n",
    "        \"time\": t,\n",
    "    }, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. prepare datasets for training and validation\n",
    "#  apply the `prepare_batch()` function to every batch\n",
    "def prepare_dataset(X, batch_size=32, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(X)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10_000)\n",
    "    return ds.batch(batch_size).map(prepare_batch).prefetch(1)\n",
    "\n",
    "train_set = prepare_dataset(X_train, batch_size=32, shuffle=True)\n",
    "valid_set = prepare_dataset(X_valid, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. just a quick sanity check\n",
    "# take a look at a few training samples, along with the corresponding \n",
    "# noise to predict, and the original images (which we get by subtracting \n",
    "# the appropriately scaled noise from the appropriately scaled noisy image):\n",
    "\n",
    "def subtract_noise(X_noisy, time, noise):\n",
    "    X_shape = tf.shape(X_noisy)\n",
    "    alpha_cm = tf.gather(alpha_cumprod, time)\n",
    "    alpha_cm = tf.reshape(alpha_cm, [X_shape[0]] + [1] * (len(X_shape) - 1))\n",
    "    return (X_noisy - (1 - alpha_cm) ** 0.5 * noise) / alpha_cm ** 0.5\n",
    "\n",
    "X_dict, Y_noise = list(train_set.take(1))[0]  # get the first batch\n",
    "X_original = subtract_noise(X_dict[\"X_noisy\"], X_dict[\"time\"], Y_noise)\n",
    "\n",
    "print(\"Original images\")\n",
    "plot_multiple_images(X_original[:8].numpy())\n",
    "plt.show()\n",
    "print(\"Time steps:\", X_dict[\"time\"].numpy()[:8])\n",
    "print(\"Noisy images\")\n",
    "plot_multiple_images(X_dict[\"X_noisy\"][:8].numpy())\n",
    "plt.show()\n",
    "print(\"Noise to predict\")\n",
    "plot_multiple_images(Y_noise[:8].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. implements a custom time encoding layer\n",
    "# It will need to process both images and times. We will encode \n",
    "# the times using a sinusoidal encoding, as suggested in the DDPM paper, \n",
    "# just like in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper. \n",
    "# Given a vector of _m_ integers representing time indices (integers), \n",
    "# the layer returns an _m_ √ó _d_ matrix, where _d_ is the chosen embedding size.\n",
    "\n",
    "embed_size = 64\n",
    "\n",
    "class TimeEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, T, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(T + 1), 2 * np.arange(embed_size // 2))\n",
    "        t_emb = np.empty((T + 1, embed_size))\n",
    "        t_emb[:, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        t_emb[:, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.time_encodings = tf.constant(t_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(self.time_encodings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. utility function used to build the diffusion model\n",
    "def build_diffusion_model():\n",
    "    X_noisy = tf.keras.layers.Input(shape=[28, 28, 1], name=\"X_noisy\")\n",
    "    time_input = tf.keras.layers.Input(shape=[], dtype=tf.int32, name=\"time\")\n",
    "    time_enc = TimeEncoding(T, embed_size)(time_input)\n",
    "\n",
    "    dim = 16\n",
    "    Z = tf.keras.layers.ZeroPadding2D((3, 3))(X_noisy)\n",
    "    Z = tf.keras.layers.Conv2D(dim, 3)(Z)\n",
    "    Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "    Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "\n",
    "    time = tf.keras.layers.Dense(dim)(time_enc)  # adapt time encoding\n",
    "    Z = time[:, tf.newaxis, tf.newaxis, :] + Z  # add time data to every pixel\n",
    "\n",
    "    skip = Z\n",
    "    cross_skips = []  # skip connections across the down & up parts of the UNet\n",
    "\n",
    "    for dim in (32, 64, 128):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.SeparableConv2D(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        cross_skips.append(Z)\n",
    "        Z = tf.keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(Z)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, strides=2,\n",
    "                                           padding=\"same\")(skip)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        skip = Z\n",
    "\n",
    "    for dim in (64, 32, 16):\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.Activation(\"relu\")(Z)\n",
    "        Z = tf.keras.layers.Conv2DTranspose(dim, 3, padding=\"same\")(Z)\n",
    "        Z = tf.keras.layers.BatchNormalization()(Z)\n",
    "\n",
    "        Z = tf.keras.layers.UpSampling2D(2)(Z)\n",
    "\n",
    "        skip_link = tf.keras.layers.UpSampling2D(2)(skip)\n",
    "        skip_link = tf.keras.layers.Conv2D(dim, 1, padding=\"same\")(skip_link)\n",
    "        Z = tf.keras.layers.add([Z, skip_link])\n",
    "\n",
    "        time = tf.keras.layers.Dense(dim)(time_enc)\n",
    "        Z = time[:, tf.newaxis, tf.newaxis, :] + Z\n",
    "        Z = tf.keras.layers.concatenate([Z, cross_skips.pop()], axis=-1)\n",
    "        skip = Z\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 3, padding=\"same\")(Z)[:, 2:-2, 2:-2]\n",
    "    return tf.keras.Model(inputs=[X_noisy, time_input], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. build the diffusion model\n",
    "model = build_diffusion_model()\n",
    "model.compile(loss=tf.keras.losses.Huber(), optimizer=\"nadam\")\n",
    "\n",
    "# adds a ModelCheckpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_diffusion_model\",\n",
    "                                                   save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. train the model\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=100,\n",
    "                    callbacks=[checkpoint_cb])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. generate images with the trained model\n",
    "def generate(model, batch_size=32):\n",
    "    X = tf.random.normal([batch_size, 28, 28, 1])\n",
    "    for t in range(T - 1, 0, -1):\n",
    "        print(f\"\\rt = {t}\", end=\" \")  # extra code ‚Äì show progress\n",
    "        noise = (tf.random.normal if t > 1 else tf.zeros)(tf.shape(X))\n",
    "        X_noise = model({\"X_noisy\": X, \"time\": tf.constant([t] * batch_size)})\n",
    "        X = (\n",
    "            1 / alpha[t] ** 0.5\n",
    "            * (X - beta[t] / (1 - alpha_cumprod[t]) ** 0.5 * X_noise)\n",
    "            + (1 - alpha[t]) ** 0.5 * noise\n",
    "        )\n",
    "    return X\n",
    "\n",
    "X_gen = generate(model)  # generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. show the generated fashions\n",
    "plot_multiple_images(X_gen.numpy(), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Semantic hashing using a binary autoencoder\n",
    "---\n",
    "- `semantic hash` identifies image's content\n",
    "- images that look alike will have the same hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a HAE\n",
    "hashing_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.GaussianNoise(15.),\n",
    "    tf.keras.layers.Dense(16, activation=\"sigmoid\"),\n",
    "])\n",
    "hashing_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "hashing_ae = tf.keras.Sequential([hashing_encoder, hashing_decoder])\n",
    "hashing_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "history = hashing_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructions(hashing_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = hashing_encoder.predict(X_valid).round().astype(np.int32)\n",
    "hashes *= np.array([[2 ** bit for bit in range(16)]])\n",
    "hashes = hashes.sum(axis=1)\n",
    "for h in hashes[:5]:\n",
    "    print(f\"{h:016b}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n_hashes = 10\n",
    "n_images = 8\n",
    "\n",
    "top_hashes = Counter(hashes).most_common(n_hashes)\n",
    "\n",
    "plt.figure(figsize=(n_images, n_hashes))\n",
    "for hash_index, (image_hash, hash_count) in enumerate(top_hashes):\n",
    "    indices = (hashes == image_hash)\n",
    "    for index, image in enumerate(X_valid[indices][:n_images]):\n",
    "        plt.subplot(n_hashes, n_images, hash_index * n_images + index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
