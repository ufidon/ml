{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod2/cmte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod2/cmte.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "__Classification__\n",
    "\n",
    "_homl3 ch3_\n",
    "\n",
    "- MNIST - a dataset of handwritten digits\n",
    "- Building a digit recognizer\n",
    "- Model evaluation\n",
    "  - Measuring Accuracy Using Cross-Validation\n",
    "  - Confusion Matrices\n",
    "  - Precision and Recall\n",
    "  - The Precision/Recall Trade-off\n",
    "  - The ROC Curve\n",
    "- Multiclass Classification\n",
    "  - Error Analysis\n",
    "- Multilabel Classification\n",
    "- Multioutput Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST - a dataset of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "---\n",
    "- Modified National Institute of Standards and Technology database (MNIST)\n",
    "- a large database of handwritten digits used by image processing systems\n",
    "- contains 70,000 black and white images \n",
    "  - 60,000 for training and 10,000 for testing\n",
    "- each image is normalized to fit into a 28x28 pixel bounding box and anti-aliased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset from https://www.openml.org/\n",
    "mnist = skds.fetch_openml('mnist_784', as_frame=False)\n",
    "\n",
    "# the returned if of type sklearn.utils.Bunch\n",
    "# this is a dictionary whose keys can also be accessed as attributes\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the description of the dataset\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist.data, mnist.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1000].reshape((28,28))), y[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(10,10,figsize=(9,9), layout='constrained')\n",
    "for idx, dimg in enumerate(X_test[:100]):\n",
    "  axs[idx//10, idx%10].imshow(dimg.reshape((28,28)), cmap='binary')\n",
    "  axs[idx//10, idx%10].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is already shuffled and split into a training set and a test set\n",
    "X_train, y_train = X[:60_000], y[:60_000]\n",
    "X_test, y_test = X[60_000:], y[60_000:]\n",
    "# üëç Thumb rule for data splitting: 80% for training 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a digit recognizer\n",
    "---\n",
    "- Let's start from recognizing a single digit such as\n",
    "  - `0` or `non-0`, `8` or `non-8`\n",
    "  - which is a binary classifier\n",
    "- can be implemented with many scikit-learn's classifiers, e.g.\n",
    "  - stochastic gradient descent (SGD, or stochastic GD) classifier\n",
    "  - implemented in the scikit-learn's SGDClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train a binary classifier to recognize 8\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clfSgd = SGDClassifier(random_state=50)\n",
    "y_train_8 = (y_train == '8')\n",
    "clfSgd.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognize 8 from test images using this classifier\n",
    "res = clfSgd.predict(X_test[:100])\n",
    "res.reshape((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1=plt.subplots(10,10,figsize=(9,9), layout='constrained')\n",
    "for idx, dimg in enumerate(X_test[:100]):\n",
    "  axs1[idx//10, idx%10].imshow(dimg.reshape((28,28)), cmap='binary') if res[idx] == False else axs1[idx//10, idx%10].imshow(dimg.reshape((28,28)))\n",
    "  axs1[idx//10, idx%10].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Model evaluation](https://scikit-learn.org/stable/model_selection.html)\n",
    "---\n",
    "- many metrics are available for model evaluation, such as\n",
    "  - confusion matrix\n",
    "  - accuracy, precision, recall, f1 score, etc.\n",
    "- which metrics are preferred depends on the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Accuracy Using Cross-Validation\n",
    "---\n",
    "- k-fold cross-validation\n",
    "  - split the training set into k folds\n",
    "  - train the model k times\n",
    "  - hold out a different fold each time for evaluation\n",
    "  - implemented with cross_val_score in scikit\n",
    "\n",
    "accuracy=(# of correct predictions)/(# of all predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clfSgd, X_train, y_train_8, cv=5, scoring='accuracy') # cv=5 number of folds, default 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracies are quite good for all folds. \n",
    "# However, this is caused by the imbalance of the chosen data.\n",
    "# by just telling not-8 every time, we get 90% right\n",
    "1-len(y_train_8[y_train_8 == True])/len(y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equally randomly guess imbalanced data achieves high accuracy\n",
    "# so accuracy is NOT useful in situations with highly imbalanced data\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clfDummy = DummyClassifier()\n",
    "clfDummy.fit(X_train, y_train_8)\n",
    "cross_val_score(clfDummy, X_train, y_train_8, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an implementation of cross-validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skFolder = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for trainIndex, testIndex in skFolder.split(X_train, y_train_8):\n",
    "  cloneClf = clone(clfSgd)\n",
    "  X_trainFold = X_train[trainIndex]\n",
    "  y_trainFold = y_train_8[trainIndex]\n",
    "  X_testFold = X_train[testIndex]\n",
    "  y_testFold = y_train_8[testIndex]\n",
    "\n",
    "  cloneClf.fit(X_trainFold, y_trainFold)\n",
    "  yPred = cloneClf.predict(X_testFold)\n",
    "  nCorrect = sum(yPred == y_testFold)\n",
    "  print(nCorrect/len(yPred), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Confusion Matrices](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "---\n",
    "- visualize of the performance of algorithms\n",
    "- show number of misclassifications\n",
    "\n",
    "| Actual\\Prediction | non-`8` | `8` |\n",
    "|:---:|:---:|:---:|\n",
    "| non-`8` | True negative (TN) | False positive (FP)<br>or type I error |\n",
    "| `8` | False negative (FN)<br>or type II error | True positive (TP) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix on training data\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(clfSgd, X_train, y_train_8)\n",
    "cm = confusion_matrix(y_train_8, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall and F1 score\n",
    "---\n",
    "\n",
    "- the precision of the classifier is the accuracy of the positive predictions\n",
    "\n",
    "\n",
    "$\\displaystyle precision=\\frac{TP}{TP+FP}$\n",
    "\n",
    "- could be misleading in the case like\n",
    "  - always make negative predictions\n",
    "  - make only one positive prediction on the instance it is sure about\n",
    "  - then, precision = 1/1 = 100%\n",
    "- so, precision is usually used along with *recall*, the *sensitivity*, or the *true positive rate (TPR)*\n",
    "  - i.e. the ratio of positive instances correctly predicted by the classifier\n",
    "\n",
    "$\\displaystyle recall=\\frac{TP}{TP+FN}$\n",
    "\n",
    "- Now, accuracy can be calculated as\n",
    "\n",
    "$\\displaystyle accuracy = \\frac{TP+TN}{P+N}=\\frac{TP+TN}{TP+TN+FP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "tn,fp,fn,tp = cm.flatten()\n",
    "print(f'precision=TP/(TP+FP)={tp}/({tp}+{fp})={tp/(tp+fp)}={precision_score(y_train_8,y_train_pred)}')\n",
    "print(f'recall=TP/(TP+FN)={tp}/({tp}+{fn})={tp/(tp+fn)}={recall_score(y_train_8,y_train_pred)}')\n",
    "print(f'accuracy=(TP+TN)/(TP+TN+FP+FN)=({tp}+{tn})/({tp}+{tn}+{fp}+{fn})={cm.trace()/cm.sum()}={accuracy_score(y_train_8,y_train_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the classifier is correct only 60.33% of the time\n",
    "  - detects 56.79% of the `8`'s\n",
    "- precision and recall can be combined into a single metric $F_1$ score\n",
    "  - the *harmonic mean* of precision and recall\n",
    "  - gives more weight to low values\n",
    "  - $F_1$ is high when both precision and recall are high\n",
    "\n",
    "$\\displaystyle F_1=\\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}=\\frac{2TP}{2TP+FN+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'f1 = 2TP/(2TP+FN+FP)=2*{tp}/(2*{tp}+{fn}+{fp})={2*tp/(2*tp+fn+fp)}={f1_score(y_train_8, y_train_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision/Recall Trade-off\n",
    "---\n",
    "- the *precision/recall trade-off* is that increasing precision reduces recall, and vice versa\n",
    "- the SGDClassifier makes its classification decisions in two steps\n",
    "  - computes a score based on a decision function\n",
    "  - compares the score with a threshold\n",
    "    - classifies the instance as positive if score > threshold\n",
    "    - else negative\n",
    "  - the default threshold used by the SGDClassifier is 0\n",
    "  - raising the threshold decreases recall\n",
    "- the appropriate threshold, i.e. the precision/recall trade-off can be made on\n",
    "  - the curves of precision vs. threshold and recall vs. threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all precisions and recalls vs. thresholds\n",
    "y_scores = cross_val_predict(clfSgd, X_train, y_train_8, method='decision_function')\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls,thresholds = precision_recall_curve(y_train_8, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the curves of precision vs. threshold and recall vs. threshold\n",
    "threshold = 3000\n",
    "fig2, ax2 = plt.subplots(figsize=(6,3),layout='constrained')\n",
    "ax2.plot(thresholds,precisions[:-1], 'b--', label='Precision', linewidth=2)\n",
    "ax2.plot(thresholds,recalls[:-1],'g-', label='Recall', linewidth=2)\n",
    "ax2.vlines(threshold,0, 1.0, 'r', \"dotted\", label='threshold')\n",
    "\n",
    "idx = (thresholds >= threshold).argmax() # first index ‚â• threshold\n",
    "ax2.plot(thresholds[idx], precisions[idx], 'bo')\n",
    "ax2.plot(thresholds[idx], recalls[idx], 'go')\n",
    "ax2.grid('on')\n",
    "ax2.axis([-50000,25000,-0.01,1.01])\n",
    "ax2.text(-5000,.05, 'Threshold')\n",
    "ax2.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A good precision/recall trade-off can also be chosen on the curve of precision vs. recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a good threshold on the curve of precision vs. recall\n",
    "fig3, ax3 = plt.subplots(figsize=(5,4),layout='constrained')\n",
    "ax3.plot(recalls, precisions, linewidth=2, label='Precision/Recall curve')\n",
    "\n",
    "ax3.plot([recalls[idx], recalls[idx]], [0, precisions[idx]], 'r:')\n",
    "ax3.plot([0, recalls[idx]], [precisions[idx], precisions[idx]], 'r:')\n",
    "ax3.plot([recalls[idx]], [precisions[idx]], 'ro', label=f'Point at threshold {threshold}')\n",
    "import matplotlib.patches as mpp\n",
    "ax3.add_patch(mpp.FancyArrowPatch((0.7,0.44), (0.6,0.6), connectionstyle='arc3,rad=.0',\n",
    "                                  arrowstyle='Simple, tail_width=2, head_width=8, head_length=10',\n",
    "                                  color='#ff0000'))\n",
    "ax3.text(.7,.52,'Higher\\nthreshold',color='#0000ff')\n",
    "ax3.axis([0,1,0,1])\n",
    "ax3.grid()\n",
    "ax3.set_xlabel('Recalls')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a good Precision/Recall (PR) curve bends toward the top-right corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the threshold giving 90% precision\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and check their precision and recall\n",
    "y_train_pred_90 = (y_scores >= threshold_for_90_precision)\n",
    "print(f'precision={precision_score(y_train_8, y_train_pred_90)}\\nrecall={recall_score(y_train_8, y_train_pred_90)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With 90% precision, however, the recall is too low to be acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "---\n",
    "- i.e. the receiver operating characteristic (ROC) curve\n",
    "  - similar to precision vs. recall curve\n",
    "- plots *true positive rate (TPR)* (another name for recall) vs. *false positive rate (FPR)*\n",
    "  - at each threshold\n",
    "- FPR is also called fall-out\n",
    "\n",
    "$\\displaystyle FPR=\\frac{FP}{FP+TN}=1-TNR$ \n",
    "\n",
    "$\\displaystyle TNR=\\frac{TN}{FP+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all fpr and tpr at each threshold\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_8, y_scores)\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax4 = plt.subplots(figsize=(5,4),layout='constrained')\n",
    "ax4.plot(fpr, tpr, linewidth=2, label='ROC curve')\n",
    "ax4.plot([0,1],[0,1], 'c:', label=\"Random classifier's ROC curve\")\n",
    "ax4.plot([fpr_90], [tpr_90], 'ro', label='Threshold for 90% precision')\n",
    "\n",
    "ax4.add_patch(mpp.FancyArrowPatch((.25,.75),(.1,.62), connectionstyle='arc3,rad=.2',\n",
    "                                  arrowstyle='Simple, tail_width=1.5, head_width=8, head_length=10',\n",
    "                                  color='#ff0000'))\n",
    "ax4.text(.2,.64,\"Higher\\nthreshold\", color='#0000ff')\n",
    "ax4.axis([0,1,0,1])\n",
    "ax4.grid()\n",
    "ax4.set_xlabel('False Positive Rate (Fall-Out)')\n",
    "ax4.set_ylabel('True Positive Rate (Recall)')\n",
    "ax4.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the ROC curve of a good classifier bends toward the top-left corner\n",
    "  - trade-off: the higher TPR, the higher FPR\n",
    "- the metric for classifier related to ROC is the *area under the curve (AUC)*\n",
    "  - area of 1 means a perfect classifier\n",
    "  - area of 0.5 means a purely random classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ROC AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(f'roc auc={roc_auc_score(y_train_8, y_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare SGDClassifier with RandomForestClassifier\n",
    "---\n",
    "- classifier can be compared with the evaluation metrics such as\n",
    "  - Precision/Recall (PR) curve, F1 score, ROC AUC score, etc.\n",
    "- RandomForestClassifier.predict_proba() calculates class probabilities for each instance\n",
    "  - the probabilities of the positive class can be used as scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=50)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_8, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the class probabilities for the first 10 images in the training set\n",
    "# each row: probability of being negative, probability of being positive\n",
    "y_probas_forest[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ues the estimated probabilities for the positive class as the scores\n",
    "y_scores_forest = y_probas_forest[:,1]\n",
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(y_train_8, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, ax5 = plt.subplots(figsize=(5,4),layout='constrained')\n",
    "ax5.plot(recalls_forest, precisions_forest, 'b-', linewidth=2, label='Random forest')\n",
    "ax5.plot(recalls, precisions, '--', linewidth=2, label='SGD')\n",
    "\n",
    "ax5.set_xlabel('Recall')\n",
    "ax5.set_ylabel('Precision')\n",
    "ax5.axis([0,1,0,1])\n",
    "ax5.grid()\n",
    "ax5.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  RandomForestClassifier's PR curve is much better than SGDClassifier's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RandomForestClassifier's f1 score, roc auc score, precision and recall\n",
    "y_train_pred_forest = y_probas_forest[:, 1] >= 0.5 \n",
    "print(f'f1 score={f1_score(y_train_8, y_train_pred_forest)}')\n",
    "print(f'roc auc score={roc_auc_score(y_train_8, y_train_pred_forest)}')\n",
    "print(f'precision score={precision_score(y_train_8, y_train_pred_forest)}')\n",
    "print(f'recall score={recall_score(y_train_8, y_train_pred_forest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- they are all better than SGDClassifier's respective scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Classification\n",
    "---\n",
    "- binary classifiers distinguish between two classes\n",
    "  - such as SGDClassifier and SVC\n",
    "  - multiple binary classifiers can be combined to classify multiple classes\n",
    "- multiclass classifiers can distinguish between more than two classes\n",
    "  - also called multinomial classifiers\n",
    "  - such as RandomForestClassifier, GaussianNB and LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two Multiclass Classification Strategies used by Binary Classifier (BC)\n",
    "---\n",
    "- one-versus-the-rest (OvR)\n",
    "   - also called one-versus-all (OvA)\n",
    "   - needs $M$ BCs, one for each class\n",
    "   - classify an instance with all classifiers and choose the class output from the classifier with the highest score\n",
    "   - preferred by most BCs\n",
    " - one-versus-one (OvO)\n",
    "   - train a BC for every pair of classes\n",
    "   - needs $\\displaystyle {{M}\\choose{2}}=\\frac{M(M-1)}{2}$ BCs\n",
    "   - each classifier only needs to be trained on the part of the training set containing the two classes that it must distinguish\n",
    "   - mainly used by BCs that scale poorly with the size of the training set\n",
    "   - rare, such as *support vector machine classifier*\n",
    " - scikit runs OvR or OvO automatically for multi-classification with BCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-classify 10 digits with SVC\n",
    "# scikit used the OvO strategy and trained 45 binary classifiers\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(random_state=50)\n",
    "svm_clf.fit(X_train[:2000], y_train[:2000]) # choose a small part of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predicts = svm_clf.predict(X_test[:100])\n",
    "print(f'{svm_predicts}\\n{svm_predicts==np.array(y_test[:100])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Error Analysis\n",
    "- Multilabel Classification\n",
    "- Multioutput Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Model selection and evaluation in scikit](https://scikit-learn.org/stable/model_selection.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
