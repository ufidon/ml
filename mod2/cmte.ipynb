{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/ml/blob/main/mod2/cmte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/ml/blob/main/mod2/cmte.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "__Classification__\n",
    "\n",
    "_homl3 ch3_\n",
    "\n",
    "- MNIST - a dataset of handwritten digits\n",
    "- Building a digit recognizer\n",
    "- Model evaluation\n",
    "  - Measuring Accuracy Using Cross-Validation\n",
    "  - Confusion Matrices\n",
    "  - Precision and Recall\n",
    "  - The Precision/Recall Trade-off\n",
    "  - The ROC Curve\n",
    "- Multiclass Classification\n",
    "  - Error Analysis\n",
    "- Multilabel Classification\n",
    "- Multioutput Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import sklearn as skl, sklearn.datasets as skds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST - a dataset of handwritten digits](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "---\n",
    "- Modified National Institute of Standards and Technology database (MNIST)\n",
    "- a large database of handwritten digits used by image processing systems\n",
    "- contains 70,000 black and white images \n",
    "  - 60,000 for training and 10,000 for testing\n",
    "- each image is normalized to fit into a 28x28 pixel bounding box and anti-aliased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset from https://www.openml.org/\n",
    "mnist = skds.fetch_openml('mnist_784', as_frame=False)\n",
    "\n",
    "# the returned if of type sklearn.utils.Bunch\n",
    "# this is a dictionary whose keys can also be accessed as attributes\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the description of the dataset\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist.data, mnist.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1000].reshape((28,28))), y[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(10,10,figsize=(9,9), layout='constrained')\n",
    "for idx, dimg in enumerate(X[60_000:60_100]):\n",
    "  axs[idx//10, idx%10].imshow(dimg.reshape((28,28)), cmap='binary')\n",
    "  axs[idx//10, idx%10].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is already shuffled and split into a training set and a test set\n",
    "X_train, y_train = X[:60_000], y[:60_000]\n",
    "X_test, y_test = X[60_000:], y[60_000:]\n",
    "# üëç Thumb rule for data splitting: 80% for training 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a digit recognizer\n",
    "---\n",
    "- Let's start from recognizing a single digit such as\n",
    "  - `0` or `non-0`, `8` or `non-8`\n",
    "  - which is a binary classifier\n",
    "- can be implemented with many scikit-learn's classifiers, e.g.\n",
    "  - stochastic gradient descent (SGD, or stochastic GD) classifier\n",
    "  - implemented in the scikit-learn's SGDClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train a binary classifier to recognize 8\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clfSgd = SGDClassifier(random_state=50)\n",
    "y_train_8 = (y_train == '8')\n",
    "clfSgd.fit(X_train, y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognize 8 from test images using this classifier\n",
    "res = clfSgd.predict(X[60_000:60_100])\n",
    "res.reshape((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1=plt.subplots(10,10,figsize=(9,9), layout='constrained')\n",
    "for idx, dimg in enumerate(X[60_000:60_100]):\n",
    "  axs1[idx//10, idx%10].imshow(dimg.reshape((28,28)), cmap='binary') if res[idx] == False else axs1[idx//10, idx%10].imshow(dimg.reshape((28,28)))\n",
    "  axs1[idx//10, idx%10].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Model evaluation](https://scikit-learn.org/stable/model_selection.html)\n",
    "---\n",
    "- many metrics are available for model evaluation, such as\n",
    "  - confusion matrix\n",
    "  - accuracy, precision, recall, f1 score, etc.\n",
    "- which metrics are preferred depends on the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Accuracy Using Cross-Validation\n",
    "---\n",
    "- k-fold cross-validation\n",
    "  - split the training set into k folds\n",
    "  - train the model k times\n",
    "  - hold out a different fold each time for evaluation\n",
    "  - implemented with cross_val_score in scikit\n",
    "\n",
    "accuracy=(# of correct predictions)/(# of all predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clfSgd, X_train, y_train_8, cv=5, scoring='accuracy') # cv=5 number of folds, default 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracies are quite good for all folds. \n",
    "# However, this is caused by the imbalance of the chosen data.\n",
    "# by just telling not-8 every time, we get 90% right\n",
    "1-len(y_train_8[y_train_8 == True])/len(y_train_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equally randomly guess imbalanced data achieves high accuracy\n",
    "# so accuracy is NOT useful in situations with highly imbalanced data\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clfDummy = DummyClassifier()\n",
    "clfDummy.fit(X_train, y_train_8)\n",
    "cross_val_score(clfDummy, X_train, y_train_8, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an implementation of cross-validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skFolder = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "for trainIndex, testIndex in skFolder.split(X_train, y_train_8):\n",
    "  cloneClf = clone(clfSgd)\n",
    "  X_trainFold = X_train[trainIndex]\n",
    "  y_trainFold = y_train_8[trainIndex]\n",
    "  X_testFold = X_train[testIndex]\n",
    "  y_testFold = y_train_8[testIndex]\n",
    "\n",
    "  cloneClf.fit(X_trainFold, y_trainFold)\n",
    "  yPred = cloneClf.predict(X_testFold)\n",
    "  nCorrect = sum(yPred == y_testFold)\n",
    "  print(nCorrect/len(yPred), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Confusion Matrices](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "---\n",
    "- visualize of the performance of algorithms\n",
    "- show number of misclassifications\n",
    "\n",
    "| Actual\\Prediction | non-`8` | `8` |\n",
    "|:---:|:---:|:---:|\n",
    "| non-`8` | True negative (TN) | False positive (FP)<br>or type I error |\n",
    "| `8` | False negative (FN)<br>or type II error | True positive (TP) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the confusion matrix on training data\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(clfSgd, X_train, y_train_8)\n",
    "cm = confusion_matrix(y_train_8, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall and F1 score\n",
    "---\n",
    "\n",
    "- the precision of the classifier is the accuracy of the positive predictions\n",
    "\n",
    "\n",
    "$\\displaystyle precision=\\frac{TP}{TP+FP}$\n",
    "\n",
    "- could be misleading in the case like\n",
    "  - always make negative predictions\n",
    "  - make only one positive prediction on the instance it is sure about\n",
    "  - then, precision = 1/1 = 100%\n",
    "- so, precision is usually used along with *recall*, the *sensitivity*, or the *true positive rate (TPR)*\n",
    "  - i.e. the ratio of positive instances correctly predicted by the classifier\n",
    "\n",
    "$\\displaystyle recall=\\frac{TP}{TP+FN}$\n",
    "\n",
    "- Now, accuracy can be calculated as\n",
    "\n",
    "$\\displaystyle accuracy = \\frac{TP+TN}{P+N}=\\frac{TP+TN}{TP+TN+FP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "tn,fp,fn,tp = cm.flatten()\n",
    "print(f'precision=TP/(TP+FP)={tp}/({tp}+{fp})={tp/(tp+fp)}={precision_score(y_train_8,y_train_pred)}')\n",
    "print(f'recall=TP/(TP+FN)={tp}/({tp}+{fn})={tp/(tp+fn)}={recall_score(y_train_8,y_train_pred)}')\n",
    "print(f'accuracy=(TP+TN)/(TP+TN+FP+FN)=({tp}+{tn})/({tp}+{tn}+{fp}+{fn})={cm.trace()/cm.sum()}={accuracy_score(y_train_8,y_train_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the classifier is correct only 60.33% of the time\n",
    "  - detects 56.79% of the `8`'s\n",
    "- precision and recall can be combined into a single metric $F_1$ score\n",
    "  - the *harmonic mean* of precision and recall\n",
    "  - gives more weight to low values\n",
    "  - $F_1$ is high when both precision and recall are high\n",
    "\n",
    "$\\displaystyle F_1=\\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}=\\frac{2TP}{2TP+FN+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'f1 = 2TP/(2TP+FN+FP)=2*{tp}/(2*{tp}+{fn}+{fp})={2*tp/(2*tp+fn+fp)}={f1_score(y_train_8, y_train_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision/Recall Trade-off\n",
    "---\n",
    "- the *precision/recall trade-off* is that increasing precision reduces recall, and vice versa\n",
    "- the SGDClassifier makes its classification decisions in two steps\n",
    "  - computes a score based on a decision function\n",
    "  - compares the score with a threshold\n",
    "    - classifies the instance as positive if score > threshold\n",
    "    - else negative\n",
    "  - the default threshold used by the SGDClassifier is 0\n",
    "  - raising the threshold decreases recall\n",
    "- the appropriate threshold, i.e. the precision/recall trade-off can be made on\n",
    "  - the curves of precision vs. threshold and recall vs. threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all precisions and recalls vs. thresholds\n",
    "y_scores = cross_val_predict(clfSgd, X_train, y_train_8, method='decision_function')\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls,thresholds = precision_recall_curve(y_train_8, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the curves of precision vs. threshold and recall vs. threshold\n",
    "threshold = 3000\n",
    "fig2, ax2 = plt.subplots(figsize=(6,3),layout='constrained')\n",
    "ax2.plot(thresholds,precisions[:-1], 'b--', label='Precision', linewidth=2)\n",
    "ax2.plot(thresholds,recalls[:-1],'g-', label='Recall', linewidth=2)\n",
    "ax2.vlines(threshold,0, 1.0, 'r', \"dotted\", label='threshold')\n",
    "\n",
    "idx = (thresholds >= threshold).argmax() # first index ‚â• threshold\n",
    "ax2.plot(thresholds[idx], precisions[idx], 'bo')\n",
    "ax2.plot(thresholds[idx], recalls[idx], 'go')\n",
    "ax2.grid('on')\n",
    "ax2.axis([-50000,25000,-0.01,1.01])\n",
    "ax2.set_xlabel(\"Threshold\")\n",
    "ax2.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiclass Classification\n",
    "  - Error Analysis\n",
    "- Multilabel Classification\n",
    "- Multioutput Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Model selection and evaluation in scikit](https://scikit-learn.org/stable/model_selection.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
